{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import SGD\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score\n",
    "from keras import regularizers\n",
    "#import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('hr_attrition.csv')\n",
    "#ibm_df = pd.read_csv('IBM_HR.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1470.000000\n",
       "mean        2.799320\n",
       "std         1.289271\n",
       "min         0.000000\n",
       "25%         2.000000\n",
       "50%         3.000000\n",
       "75%         3.000000\n",
       "max         6.000000\n",
       "Name: TrainingTimesLastYear, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training = df['TrainingTimesLastYear']\n",
    "training.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "to_drop = ['Attrition', 'EmployeeCount','Over18', 'StandardHours', 'EmployeeNumber',]\n",
    "labels = ['Attrition']\n",
    "\n",
    "wages = ['DailyRate', 'HourlyRate', 'MonthlyIncome', 'MonthlyRate']\n",
    "\n",
    "to_dummy = ['BusinessTravel', 'Department', 'EducationField', 'Gender', 'JobRole', 'MaritalStatus', 'OverTime']##7\n",
    "\n",
    "categorical = ['Education', 'EnvironmentSatisfaction', 'JobInvolvement', 'JobLevel', 'JobSatisfaction', \n",
    "               'PerformanceRating', 'RelationshipSatisfaction', 'StockOptionLevel', 'WorkLifeBalance'] ##9\n",
    "\n",
    "to_continous = ['Age', 'DistanceFromHome', 'HourlyRate', 'NumCompaniesWorked', 'PercentSalaryHike',\n",
    "             'TotalWorkingYears', 'TrainingTimesLastYear', 'YearsAtCompany', 'YearsInCurrentRole', \n",
    "             'YearsSinceLastPromotion', 'YearsWithCurrManager', 'Education', 'EnvironmentSatisfaction',\n",
    "                'JobInvolvement', 'JobLevel', 'JobSatisfaction', \n",
    "               'PerformanceRating', 'RelationshipSatisfaction', 'StockOptionLevel', 'WorkLifeBalance'] ##11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Age', 'Attrition', 'BusinessTravel', 'DailyRate', 'Department',\n",
       "       'DistanceFromHome', 'Education', 'EducationField', 'EmployeeCount',\n",
       "       'EmployeeNumber', 'EnvironmentSatisfaction', 'Gender', 'HourlyRate',\n",
       "       'JobInvolvement', 'JobLevel', 'JobRole', 'JobSatisfaction',\n",
       "       'MaritalStatus', 'MonthlyIncome', 'MonthlyRate',\n",
       "       'NumCompaniesWorked', 'Over18', 'OverTime', 'PercentSalaryHike',\n",
       "       'PerformanceRating', 'RelationshipSatisfaction', 'StandardHours',\n",
       "       'StockOptionLevel', 'TotalWorkingYears', 'TrainingTimesLastYear',\n",
       "       'WorkLifeBalance', 'YearsAtCompany', 'YearsInCurrentRole',\n",
       "       'YearsSinceLastPromotion', 'YearsWithCurrManager'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###Get labels, drop \n",
    "#labels = df['Attrition']\n",
    "#df = df.drop(to_drop, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###Get labels\n",
    "def get_labels(df):\n",
    "    labels = df['Attrition']\n",
    "    labels = [1 if i == 'Yes' else 0 for i in labels]\n",
    "    \n",
    "    return labels\n",
    "\n",
    "labels = get_labels(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###Drop labels and others\n",
    "\n",
    "to_drop = ['DailyRate', 'MonthlyIncome', 'MonthlyRate', 'Attrition',\n",
    "           'EmployeeCount','Over18', 'StandardHours', 'EmployeeNumber']\n",
    "\n",
    "def drop_from_df(df, to_drop):\n",
    "    df = df.drop(to_drop, axis = 1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = drop_from_df(df, to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Age', 'BusinessTravel', 'Department', 'DistanceFromHome',\n",
       "       'Education', 'EducationField', 'EnvironmentSatisfaction', 'Gender',\n",
       "       'HourlyRate', 'JobInvolvement', 'JobLevel', 'JobRole',\n",
       "       'JobSatisfaction', 'MaritalStatus', 'NumCompaniesWorked',\n",
       "       'OverTime', 'PercentSalaryHike', 'PerformanceRating',\n",
       "       'RelationshipSatisfaction', 'StockOptionLevel', 'TotalWorkingYears',\n",
       "       'TrainingTimesLastYear', 'WorkLifeBalance', 'YearsAtCompany',\n",
       "       'YearsInCurrentRole', 'YearsSinceLastPromotion',\n",
       "       'YearsWithCurrManager'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###Get Dummies and drop dummy features\n",
    "def get_dummies(df):\n",
    "    dummies = pd.get_dummies(df[to_dummy])\n",
    "    \n",
    "    #print(dummies)\n",
    "    df = df.drop(to_dummy, axis = 1)\n",
    "    \n",
    "    frames = [df, dummies]\n",
    "    df = pd.concat(frames, axis = 1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "df_with_dummies = get_dummies(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_columns = df_with_dummies.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dummies = pd.get_dummies(df[to_dummy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###Scale continous variables\n",
    "def get_scaled(df, continous):\n",
    "    scaler = MinMaxScaler((0.05, 0.95))\n",
    "    df[continous] = scaler.fit_transform(df[continous])\n",
    "    \n",
    "    return df\n",
    "\n",
    "dummy_scaled_df = get_scaled(df_with_dummies, to_continous)\n",
    "#all_scaled = get_scaled(df_with_dummies, all_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#scaler = MinMaxScaler((0.05, 0.95))\n",
    "#df[to_continous] = scaler.fit_transform(df[to_continous])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df = df.drop(to_dummy, axis = 1)\n",
    "#df = df.drop(['DailyRate'], axis = 1)\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#frames = [df, dummies]\n",
    "#features = pd.concat(frames, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#features = features.drop(to_dummy, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Age', 'DistanceFromHome', 'Education', 'EnvironmentSatisfaction',\n",
       "       'HourlyRate', 'JobInvolvement', 'JobLevel', 'JobSatisfaction',\n",
       "       'NumCompaniesWorked', 'PercentSalaryHike', 'PerformanceRating',\n",
       "       'RelationshipSatisfaction', 'StockOptionLevel', 'TotalWorkingYears',\n",
       "       'TrainingTimesLastYear', 'WorkLifeBalance', 'YearsAtCompany',\n",
       "       'YearsInCurrentRole', 'YearsSinceLastPromotion',\n",
       "       'YearsWithCurrManager', 'BusinessTravel_Non-Travel',\n",
       "       'BusinessTravel_Travel_Frequently', 'BusinessTravel_Travel_Rarely',\n",
       "       'Department_Human Resources', 'Department_Research & Development',\n",
       "       'Department_Sales', 'EducationField_Human Resources',\n",
       "       'EducationField_Life Sciences', 'EducationField_Marketing',\n",
       "       'EducationField_Medical', 'EducationField_Other',\n",
       "       'EducationField_Technical Degree', 'Gender_Female', 'Gender_Male',\n",
       "       'JobRole_Healthcare Representative', 'JobRole_Human Resources',\n",
       "       'JobRole_Laboratory Technician', 'JobRole_Manager',\n",
       "       'JobRole_Manufacturing Director', 'JobRole_Research Director',\n",
       "       'JobRole_Research Scientist', 'JobRole_Sales Executive',\n",
       "       'JobRole_Sales Representative', 'MaritalStatus_Divorced',\n",
       "       'MaritalStatus_Married', 'MaritalStatus_Single', 'OverTime_No',\n",
       "       'OverTime_Yes'], dtype=object)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_scaled_df.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>DistanceFromHome</th>\n",
       "      <th>Education</th>\n",
       "      <th>EnvironmentSatisfaction</th>\n",
       "      <th>HourlyRate</th>\n",
       "      <th>JobInvolvement</th>\n",
       "      <th>JobLevel</th>\n",
       "      <th>JobSatisfaction</th>\n",
       "      <th>NumCompaniesWorked</th>\n",
       "      <th>PercentSalaryHike</th>\n",
       "      <th>...</th>\n",
       "      <th>JobRole_Manufacturing Director</th>\n",
       "      <th>JobRole_Research Director</th>\n",
       "      <th>JobRole_Research Scientist</th>\n",
       "      <th>JobRole_Sales Executive</th>\n",
       "      <th>JobRole_Sales Representative</th>\n",
       "      <th>MaritalStatus_Divorced</th>\n",
       "      <th>MaritalStatus_Married</th>\n",
       "      <th>MaritalStatus_Single</th>\n",
       "      <th>OverTime_No</th>\n",
       "      <th>OverTime_Yes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.542857</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.275</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.872857</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.275</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.275</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.448571</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.275</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.821429</td>\n",
       "      <td>...</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Age  DistanceFromHome  Education  EnvironmentSatisfaction  HourlyRate  \\\n",
       "0  0.542857             0.050      0.275                     0.35    0.872857   \n",
       "1  0.714286             0.275      0.050                     0.65    0.448571   \n",
       "\n",
       "   JobInvolvement  JobLevel  JobSatisfaction  NumCompaniesWorked  \\\n",
       "0            0.65     0.275             0.95                0.85   \n",
       "1            0.35     0.275             0.35                0.15   \n",
       "\n",
       "   PercentSalaryHike      ...       JobRole_Manufacturing Director  \\\n",
       "0           0.050000      ...                                 0.05   \n",
       "1           0.821429      ...                                 0.05   \n",
       "\n",
       "   JobRole_Research Director  JobRole_Research Scientist  \\\n",
       "0                       0.05                        0.05   \n",
       "1                       0.05                        0.95   \n",
       "\n",
       "   JobRole_Sales Executive  JobRole_Sales Representative  \\\n",
       "0                     0.95                          0.05   \n",
       "1                     0.05                          0.05   \n",
       "\n",
       "   MaritalStatus_Divorced  MaritalStatus_Married  MaritalStatus_Single  \\\n",
       "0                    0.05                   0.05                  0.95   \n",
       "1                    0.05                   0.95                  0.05   \n",
       "\n",
       "   OverTime_No  OverTime_Yes  \n",
       "0         0.05          0.95  \n",
       "1         0.95          0.05  \n",
       "\n",
       "[2 rows x 48 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_scaled_df[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#labels = [1 if i == 'Yes' else 0 for i in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(dummy_scaled_df, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = pd.DataFrame(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#x_train = np.array(x_train)\n",
    "#y_train = np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_classes = 1\n",
    "input_shape = dummy_scaled_df.shape[1]\n",
    "activations = 'tanh'\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(input_shape, input_shape = (input_shape,), activation = activations,\n",
    "                kernel_regularizer = regularizers.l2(.001)))\n",
    "model.add(Dropout(0.6))\n",
    "model.add(Dense(64, activation = 'relu', kernel_regularizer = regularizers.l2(.001)))#1024\n",
    "model.add(Dropout(0.6))\n",
    "model.add(Dense(64, activation = 'relu', kernel_regularizer = regularizers.l2(.001)))#1024\n",
    "model.add(Dropout(0.6))\n",
    "model.add(Dense(num_classes, activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 48)                2352      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 48)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                3136      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 9,713\n",
      "Trainable params: 9,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss = 'binary_crossentropy', optimizer = SGD(lr = 0.01, momentum = 0.9), metrics = ['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(filepath = 'weightsz.hdf5', save_best_only = True, monitor='val_loss') \n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=50, min_lr=0.0001,\n",
    "                              cooldown=100, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1058 samples, validate on 118 samples\n",
      "Epoch 1/300\n",
      "1058/1058 [==============================] - 1s 1ms/step - loss: 0.7361 - acc: 0.7599 - val_loss: 0.6743 - val_acc: 0.7966\n",
      "Epoch 2/300\n",
      "1058/1058 [==============================] - 0s 154us/step - loss: 0.6374 - acc: 0.8374 - val_loss: 0.6591 - val_acc: 0.7966\n",
      "Epoch 3/300\n",
      "1058/1058 [==============================] - 0s 160us/step - loss: 0.6205 - acc: 0.8355 - val_loss: 0.6442 - val_acc: 0.7966\n",
      "Epoch 4/300\n",
      "1058/1058 [==============================] - 0s 155us/step - loss: 0.6105 - acc: 0.8355 - val_loss: 0.6393 - val_acc: 0.7966\n",
      "Epoch 5/300\n",
      "1058/1058 [==============================] - 0s 159us/step - loss: 0.6006 - acc: 0.8355 - val_loss: 0.6242 - val_acc: 0.7966\n",
      "Epoch 6/300\n",
      "1058/1058 [==============================] - 0s 155us/step - loss: 0.5891 - acc: 0.8355 - val_loss: 0.6190 - val_acc: 0.7966\n",
      "Epoch 7/300\n",
      "1058/1058 [==============================] - 0s 153us/step - loss: 0.5995 - acc: 0.8355 - val_loss: 0.6107 - val_acc: 0.7966\n",
      "Epoch 8/300\n",
      "1058/1058 [==============================] - 0s 155us/step - loss: 0.5798 - acc: 0.8355 - val_loss: 0.6013 - val_acc: 0.7966\n",
      "Epoch 9/300\n",
      "1058/1058 [==============================] - 0s 154us/step - loss: 0.5629 - acc: 0.8355 - val_loss: 0.5932 - val_acc: 0.7966\n",
      "Epoch 10/300\n",
      "1058/1058 [==============================] - 0s 155us/step - loss: 0.5655 - acc: 0.8355 - val_loss: 0.5882 - val_acc: 0.7966\n",
      "Epoch 11/300\n",
      "1058/1058 [==============================] - 0s 153us/step - loss: 0.5554 - acc: 0.8355 - val_loss: 0.5739 - val_acc: 0.7966\n",
      "Epoch 12/300\n",
      "1058/1058 [==============================] - 0s 165us/step - loss: 0.5548 - acc: 0.8355 - val_loss: 0.5715 - val_acc: 0.7966\n",
      "Epoch 13/300\n",
      "1058/1058 [==============================] - 0s 164us/step - loss: 0.5593 - acc: 0.8355 - val_loss: 0.5766 - val_acc: 0.7966\n",
      "Epoch 14/300\n",
      "1058/1058 [==============================] - 0s 159us/step - loss: 0.5418 - acc: 0.8355 - val_loss: 0.5594 - val_acc: 0.7966\n",
      "Epoch 15/300\n",
      "1058/1058 [==============================] - 0s 176us/step - loss: 0.5370 - acc: 0.8355 - val_loss: 0.5564 - val_acc: 0.7966\n",
      "Epoch 16/300\n",
      "1058/1058 [==============================] - 0s 169us/step - loss: 0.5304 - acc: 0.8355 - val_loss: 0.5495 - val_acc: 0.7966\n",
      "Epoch 17/300\n",
      "1058/1058 [==============================] - 0s 157us/step - loss: 0.5207 - acc: 0.8355 - val_loss: 0.5483 - val_acc: 0.7966\n",
      "Epoch 18/300\n",
      "1058/1058 [==============================] - 0s 157us/step - loss: 0.5341 - acc: 0.8355 - val_loss: 0.5453 - val_acc: 0.7966\n",
      "Epoch 19/300\n",
      "1058/1058 [==============================] - 0s 156us/step - loss: 0.5116 - acc: 0.8355 - val_loss: 0.5379 - val_acc: 0.7966\n",
      "Epoch 20/300\n",
      "1058/1058 [==============================] - 0s 165us/step - loss: 0.5084 - acc: 0.8365 - val_loss: 0.5263 - val_acc: 0.7966\n",
      "Epoch 21/300\n",
      "1058/1058 [==============================] - 0s 153us/step - loss: 0.5265 - acc: 0.8365 - val_loss: 0.5288 - val_acc: 0.7966\n",
      "Epoch 22/300\n",
      "1058/1058 [==============================] - 0s 161us/step - loss: 0.5038 - acc: 0.8384 - val_loss: 0.5198 - val_acc: 0.7966\n",
      "Epoch 23/300\n",
      "1058/1058 [==============================] - 0s 153us/step - loss: 0.4917 - acc: 0.8346 - val_loss: 0.5213 - val_acc: 0.7966\n",
      "Epoch 24/300\n",
      "1058/1058 [==============================] - 0s 160us/step - loss: 0.4981 - acc: 0.8365 - val_loss: 0.5096 - val_acc: 0.7966\n",
      "Epoch 25/300\n",
      "1058/1058 [==============================] - 0s 154us/step - loss: 0.4792 - acc: 0.8374 - val_loss: 0.5063 - val_acc: 0.7966\n",
      "Epoch 26/300\n",
      "1058/1058 [==============================] - 0s 155us/step - loss: 0.4887 - acc: 0.8459 - val_loss: 0.5019 - val_acc: 0.7966\n",
      "Epoch 27/300\n",
      "1058/1058 [==============================] - 0s 155us/step - loss: 0.4841 - acc: 0.8412 - val_loss: 0.5114 - val_acc: 0.8305\n",
      "Epoch 28/300\n",
      "1058/1058 [==============================] - 0s 155us/step - loss: 0.4875 - acc: 0.8431 - val_loss: 0.4925 - val_acc: 0.8136\n",
      "Epoch 29/300\n",
      "1058/1058 [==============================] - 0s 157us/step - loss: 0.4689 - acc: 0.8478 - val_loss: 0.4873 - val_acc: 0.8305\n",
      "Epoch 30/300\n",
      "1058/1058 [==============================] - 0s 181us/step - loss: 0.4839 - acc: 0.8431 - val_loss: 0.4935 - val_acc: 0.8559\n",
      "Epoch 31/300\n",
      "1058/1058 [==============================] - 0s 162us/step - loss: 0.4574 - acc: 0.8422 - val_loss: 0.4860 - val_acc: 0.8559\n",
      "Epoch 32/300\n",
      "1058/1058 [==============================] - 0s 158us/step - loss: 0.4657 - acc: 0.8526 - val_loss: 0.4821 - val_acc: 0.8390\n",
      "Epoch 33/300\n",
      "1058/1058 [==============================] - 0s 162us/step - loss: 0.4649 - acc: 0.8440 - val_loss: 0.4822 - val_acc: 0.8390\n",
      "Epoch 34/300\n",
      "1058/1058 [==============================] - 0s 157us/step - loss: 0.4742 - acc: 0.8554 - val_loss: 0.4787 - val_acc: 0.8390\n",
      "Epoch 35/300\n",
      "1058/1058 [==============================] - 0s 158us/step - loss: 0.4627 - acc: 0.8535 - val_loss: 0.4765 - val_acc: 0.8390\n",
      "Epoch 36/300\n",
      "1058/1058 [==============================] - 0s 162us/step - loss: 0.4578 - acc: 0.8573 - val_loss: 0.4767 - val_acc: 0.8305\n",
      "Epoch 37/300\n",
      "1058/1058 [==============================] - 0s 156us/step - loss: 0.4644 - acc: 0.8497 - val_loss: 0.4767 - val_acc: 0.8305\n",
      "Epoch 38/300\n",
      "1058/1058 [==============================] - 0s 160us/step - loss: 0.4665 - acc: 0.8526 - val_loss: 0.4741 - val_acc: 0.8475\n",
      "Epoch 39/300\n",
      "1058/1058 [==============================] - 0s 157us/step - loss: 0.4332 - acc: 0.8535 - val_loss: 0.4674 - val_acc: 0.8475\n",
      "Epoch 40/300\n",
      "1058/1058 [==============================] - 0s 160us/step - loss: 0.4444 - acc: 0.8620 - val_loss: 0.4709 - val_acc: 0.8475\n",
      "Epoch 41/300\n",
      "1058/1058 [==============================] - 0s 166us/step - loss: 0.4374 - acc: 0.8592 - val_loss: 0.4684 - val_acc: 0.8475\n",
      "Epoch 42/300\n",
      "1058/1058 [==============================] - 0s 162us/step - loss: 0.4425 - acc: 0.8582 - val_loss: 0.4677 - val_acc: 0.8136\n",
      "Epoch 43/300\n",
      "1058/1058 [==============================] - 0s 180us/step - loss: 0.4388 - acc: 0.8639 - val_loss: 0.4617 - val_acc: 0.8305\n",
      "Epoch 44/300\n",
      "1058/1058 [==============================] - 0s 166us/step - loss: 0.4320 - acc: 0.8771 - val_loss: 0.4612 - val_acc: 0.7966\n",
      "Epoch 45/300\n",
      "1058/1058 [==============================] - 0s 159us/step - loss: 0.4209 - acc: 0.8592 - val_loss: 0.4563 - val_acc: 0.8644\n",
      "Epoch 46/300\n",
      "1058/1058 [==============================] - 0s 156us/step - loss: 0.4347 - acc: 0.8582 - val_loss: 0.4540 - val_acc: 0.8559\n",
      "Epoch 47/300\n",
      "1058/1058 [==============================] - 0s 170us/step - loss: 0.4201 - acc: 0.8677 - val_loss: 0.4494 - val_acc: 0.8475\n",
      "Epoch 48/300\n",
      "1058/1058 [==============================] - 0s 164us/step - loss: 0.4317 - acc: 0.8686 - val_loss: 0.4451 - val_acc: 0.8475\n",
      "Epoch 49/300\n",
      "1058/1058 [==============================] - 0s 173us/step - loss: 0.4313 - acc: 0.8582 - val_loss: 0.4455 - val_acc: 0.8559\n",
      "Epoch 50/300\n",
      "1058/1058 [==============================] - 0s 187us/step - loss: 0.4232 - acc: 0.8696 - val_loss: 0.4437 - val_acc: 0.8475\n",
      "Epoch 51/300\n",
      "1058/1058 [==============================] - 0s 187us/step - loss: 0.4344 - acc: 0.8592 - val_loss: 0.4432 - val_acc: 0.8475\n",
      "Epoch 52/300\n",
      "1058/1058 [==============================] - 0s 172us/step - loss: 0.4200 - acc: 0.8752 - val_loss: 0.4430 - val_acc: 0.8559\n",
      "Epoch 53/300\n",
      "1058/1058 [==============================] - 0s 155us/step - loss: 0.4152 - acc: 0.8620 - val_loss: 0.4436 - val_acc: 0.8475\n",
      "Epoch 54/300\n",
      "1058/1058 [==============================] - 0s 160us/step - loss: 0.4154 - acc: 0.8733 - val_loss: 0.4466 - val_acc: 0.8305\n",
      "Epoch 55/300\n",
      "1058/1058 [==============================] - 0s 156us/step - loss: 0.4204 - acc: 0.8667 - val_loss: 0.4419 - val_acc: 0.8475\n",
      "Epoch 56/300\n",
      "1058/1058 [==============================] - 0s 155us/step - loss: 0.4187 - acc: 0.8696 - val_loss: 0.4388 - val_acc: 0.8475\n",
      "Epoch 57/300\n",
      "1058/1058 [==============================] - 0s 165us/step - loss: 0.4174 - acc: 0.8573 - val_loss: 0.4340 - val_acc: 0.8390\n",
      "Epoch 58/300\n",
      "1058/1058 [==============================] - 0s 160us/step - loss: 0.4052 - acc: 0.8696 - val_loss: 0.4348 - val_acc: 0.8305\n",
      "Epoch 59/300\n",
      "1058/1058 [==============================] - 0s 157us/step - loss: 0.4096 - acc: 0.8715 - val_loss: 0.4312 - val_acc: 0.8475\n",
      "Epoch 60/300\n",
      "1058/1058 [==============================] - 0s 156us/step - loss: 0.4035 - acc: 0.8667 - val_loss: 0.4318 - val_acc: 0.8475\n",
      "Epoch 61/300\n",
      "1058/1058 [==============================] - 0s 156us/step - loss: 0.4217 - acc: 0.8705 - val_loss: 0.4301 - val_acc: 0.8475\n",
      "Epoch 62/300\n",
      "1058/1058 [==============================] - 0s 154us/step - loss: 0.3963 - acc: 0.8771 - val_loss: 0.4311 - val_acc: 0.8475\n",
      "Epoch 63/300\n",
      "1058/1058 [==============================] - 0s 153us/step - loss: 0.4016 - acc: 0.8705 - val_loss: 0.4274 - val_acc: 0.8390\n",
      "Epoch 64/300\n",
      "1058/1058 [==============================] - 0s 154us/step - loss: 0.4042 - acc: 0.8790 - val_loss: 0.4294 - val_acc: 0.8390\n",
      "Epoch 65/300\n",
      "1058/1058 [==============================] - 0s 155us/step - loss: 0.4086 - acc: 0.8733 - val_loss: 0.4266 - val_acc: 0.8390\n",
      "Epoch 66/300\n",
      "1058/1058 [==============================] - 0s 153us/step - loss: 0.4196 - acc: 0.8696 - val_loss: 0.4236 - val_acc: 0.8390\n",
      "Epoch 67/300\n",
      "1058/1058 [==============================] - 0s 155us/step - loss: 0.4009 - acc: 0.8686 - val_loss: 0.4252 - val_acc: 0.8475\n",
      "Epoch 68/300\n",
      "1058/1058 [==============================] - 0s 156us/step - loss: 0.3897 - acc: 0.8724 - val_loss: 0.4241 - val_acc: 0.8559\n",
      "Epoch 69/300\n",
      "1058/1058 [==============================] - 0s 155us/step - loss: 0.3943 - acc: 0.8771 - val_loss: 0.4257 - val_acc: 0.8559\n",
      "Epoch 70/300\n",
      "1058/1058 [==============================] - 0s 156us/step - loss: 0.3900 - acc: 0.8752 - val_loss: 0.4231 - val_acc: 0.8305\n",
      "Epoch 71/300\n",
      "1058/1058 [==============================] - 0s 155us/step - loss: 0.3949 - acc: 0.8743 - val_loss: 0.4164 - val_acc: 0.8644\n",
      "Epoch 72/300\n",
      "1058/1058 [==============================] - 0s 156us/step - loss: 0.4004 - acc: 0.8667 - val_loss: 0.4169 - val_acc: 0.8729\n",
      "Epoch 73/300\n",
      "1058/1058 [==============================] - 0s 166us/step - loss: 0.3937 - acc: 0.8696 - val_loss: 0.4161 - val_acc: 0.8559\n",
      "Epoch 74/300\n",
      "1058/1058 [==============================] - 0s 155us/step - loss: 0.3830 - acc: 0.8752 - val_loss: 0.4179 - val_acc: 0.8475\n",
      "Epoch 75/300\n",
      "1058/1058 [==============================] - 0s 174us/step - loss: 0.3893 - acc: 0.8752 - val_loss: 0.4194 - val_acc: 0.8475\n",
      "Epoch 76/300\n",
      "1058/1058 [==============================] - 0s 167us/step - loss: 0.3844 - acc: 0.8828 - val_loss: 0.4194 - val_acc: 0.8390\n",
      "Epoch 77/300\n",
      "1058/1058 [==============================] - 0s 175us/step - loss: 0.3988 - acc: 0.8677 - val_loss: 0.4169 - val_acc: 0.8644\n",
      "Epoch 78/300\n",
      "1058/1058 [==============================] - 0s 167us/step - loss: 0.3855 - acc: 0.8752 - val_loss: 0.4126 - val_acc: 0.8475\n",
      "Epoch 79/300\n",
      "1058/1058 [==============================] - 0s 159us/step - loss: 0.3968 - acc: 0.8743 - val_loss: 0.4114 - val_acc: 0.8559\n",
      "Epoch 80/300\n",
      "1058/1058 [==============================] - 0s 156us/step - loss: 0.3739 - acc: 0.8800 - val_loss: 0.4133 - val_acc: 0.8475\n",
      "Epoch 81/300\n",
      "1058/1058 [==============================] - 0s 168us/step - loss: 0.3711 - acc: 0.8819 - val_loss: 0.4135 - val_acc: 0.8305\n",
      "Epoch 82/300\n",
      "1058/1058 [==============================] - 0s 164us/step - loss: 0.3769 - acc: 0.8762 - val_loss: 0.4091 - val_acc: 0.8559\n",
      "Epoch 83/300\n",
      "1058/1058 [==============================] - 0s 163us/step - loss: 0.3720 - acc: 0.8781 - val_loss: 0.4125 - val_acc: 0.8559\n",
      "Epoch 84/300\n",
      "1058/1058 [==============================] - 0s 169us/step - loss: 0.3686 - acc: 0.8875 - val_loss: 0.4140 - val_acc: 0.8475\n",
      "Epoch 85/300\n",
      "1058/1058 [==============================] - 0s 181us/step - loss: 0.3763 - acc: 0.8781 - val_loss: 0.4205 - val_acc: 0.8390\n",
      "Epoch 86/300\n",
      "1058/1058 [==============================] - 0s 184us/step - loss: 0.3733 - acc: 0.8762 - val_loss: 0.4083 - val_acc: 0.8559\n",
      "Epoch 87/300\n",
      "1058/1058 [==============================] - 0s 181us/step - loss: 0.3672 - acc: 0.8809 - val_loss: 0.4068 - val_acc: 0.8475\n",
      "Epoch 88/300\n",
      "1058/1058 [==============================] - 0s 155us/step - loss: 0.3755 - acc: 0.8875 - val_loss: 0.4078 - val_acc: 0.8559\n",
      "Epoch 89/300\n",
      "1058/1058 [==============================] - 0s 167us/step - loss: 0.3793 - acc: 0.8875 - val_loss: 0.4050 - val_acc: 0.8559\n",
      "Epoch 90/300\n",
      "1058/1058 [==============================] - 0s 162us/step - loss: 0.3709 - acc: 0.8894 - val_loss: 0.4035 - val_acc: 0.8644\n",
      "Epoch 91/300\n",
      "1058/1058 [==============================] - 0s 161us/step - loss: 0.3762 - acc: 0.8828 - val_loss: 0.4058 - val_acc: 0.8390\n",
      "Epoch 92/300\n",
      "1058/1058 [==============================] - 0s 157us/step - loss: 0.3720 - acc: 0.8752 - val_loss: 0.4005 - val_acc: 0.8475\n",
      "Epoch 93/300\n",
      "1058/1058 [==============================] - 0s 177us/step - loss: 0.3801 - acc: 0.8800 - val_loss: 0.4011 - val_acc: 0.8559\n",
      "Epoch 94/300\n",
      "1058/1058 [==============================] - 0s 164us/step - loss: 0.3671 - acc: 0.8800 - val_loss: 0.4044 - val_acc: 0.8559\n",
      "Epoch 95/300\n",
      "1058/1058 [==============================] - 0s 177us/step - loss: 0.3747 - acc: 0.8819 - val_loss: 0.4013 - val_acc: 0.8559\n",
      "Epoch 96/300\n",
      "1058/1058 [==============================] - 0s 170us/step - loss: 0.3719 - acc: 0.8724 - val_loss: 0.3995 - val_acc: 0.8559\n",
      "Epoch 97/300\n",
      "1058/1058 [==============================] - 0s 162us/step - loss: 0.3725 - acc: 0.8800 - val_loss: 0.3973 - val_acc: 0.8644\n",
      "Epoch 98/300\n",
      "1058/1058 [==============================] - 0s 170us/step - loss: 0.3609 - acc: 0.8847 - val_loss: 0.4010 - val_acc: 0.8559\n",
      "Epoch 99/300\n",
      "1058/1058 [==============================] - 0s 175us/step - loss: 0.3793 - acc: 0.8724 - val_loss: 0.4002 - val_acc: 0.8729\n",
      "Epoch 100/300\n",
      "1058/1058 [==============================] - 0s 157us/step - loss: 0.3814 - acc: 0.8809 - val_loss: 0.3996 - val_acc: 0.8644\n",
      "Epoch 101/300\n",
      "1058/1058 [==============================] - 0s 154us/step - loss: 0.3635 - acc: 0.8885 - val_loss: 0.3986 - val_acc: 0.8729\n",
      "Epoch 102/300\n",
      "1058/1058 [==============================] - 0s 176us/step - loss: 0.3714 - acc: 0.8696 - val_loss: 0.3997 - val_acc: 0.8559\n",
      "Epoch 103/300\n",
      "1058/1058 [==============================] - 0s 170us/step - loss: 0.3622 - acc: 0.8800 - val_loss: 0.4080 - val_acc: 0.8644\n",
      "Epoch 104/300\n",
      "1058/1058 [==============================] - 0s 167us/step - loss: 0.3565 - acc: 0.8875 - val_loss: 0.3974 - val_acc: 0.8475\n",
      "Epoch 105/300\n",
      "1058/1058 [==============================] - 0s 175us/step - loss: 0.3682 - acc: 0.8733 - val_loss: 0.3996 - val_acc: 0.8475\n",
      "Epoch 106/300\n",
      "1058/1058 [==============================] - 0s 174us/step - loss: 0.3577 - acc: 0.8800 - val_loss: 0.3975 - val_acc: 0.8475\n",
      "Epoch 107/300\n",
      "1058/1058 [==============================] - 0s 157us/step - loss: 0.3504 - acc: 0.8875 - val_loss: 0.4033 - val_acc: 0.8559\n",
      "Epoch 108/300\n",
      "1058/1058 [==============================] - 0s 168us/step - loss: 0.3543 - acc: 0.8724 - val_loss: 0.3967 - val_acc: 0.8475\n",
      "Epoch 109/300\n",
      "1058/1058 [==============================] - 0s 173us/step - loss: 0.3587 - acc: 0.8828 - val_loss: 0.3970 - val_acc: 0.8559\n",
      "Epoch 110/300\n",
      "1058/1058 [==============================] - 0s 157us/step - loss: 0.3558 - acc: 0.8932 - val_loss: 0.4005 - val_acc: 0.8475\n",
      "Epoch 111/300\n",
      "1058/1058 [==============================] - 0s 155us/step - loss: 0.3643 - acc: 0.8781 - val_loss: 0.3937 - val_acc: 0.8475\n",
      "Epoch 112/300\n",
      "1058/1058 [==============================] - 0s 172us/step - loss: 0.3454 - acc: 0.8904 - val_loss: 0.3956 - val_acc: 0.8644\n",
      "Epoch 113/300\n",
      "1058/1058 [==============================] - 0s 171us/step - loss: 0.3550 - acc: 0.8800 - val_loss: 0.3920 - val_acc: 0.8644\n",
      "Epoch 114/300\n",
      "1058/1058 [==============================] - 0s 156us/step - loss: 0.3549 - acc: 0.8866 - val_loss: 0.3945 - val_acc: 0.8644\n",
      "Epoch 115/300\n",
      "1058/1058 [==============================] - 0s 155us/step - loss: 0.3490 - acc: 0.8932 - val_loss: 0.3994 - val_acc: 0.8559\n",
      "Epoch 116/300\n",
      "1058/1058 [==============================] - 0s 157us/step - loss: 0.3417 - acc: 0.8837 - val_loss: 0.4133 - val_acc: 0.8390\n",
      "Epoch 117/300\n",
      "1058/1058 [==============================] - 0s 160us/step - loss: 0.3675 - acc: 0.8762 - val_loss: 0.4021 - val_acc: 0.8475\n",
      "Epoch 118/300\n",
      "1058/1058 [==============================] - 0s 163us/step - loss: 0.3436 - acc: 0.8781 - val_loss: 0.4064 - val_acc: 0.8390\n",
      "Epoch 119/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1058/1058 [==============================] - 0s 153us/step - loss: 0.3683 - acc: 0.8790 - val_loss: 0.3909 - val_acc: 0.8644\n",
      "Epoch 120/300\n",
      "1058/1058 [==============================] - 0s 153us/step - loss: 0.3471 - acc: 0.8837 - val_loss: 0.3962 - val_acc: 0.8644\n",
      "Epoch 121/300\n",
      "1058/1058 [==============================] - 0s 157us/step - loss: 0.3626 - acc: 0.8771 - val_loss: 0.3888 - val_acc: 0.8475\n",
      "Epoch 122/300\n",
      "1058/1058 [==============================] - 0s 155us/step - loss: 0.3465 - acc: 0.8856 - val_loss: 0.3914 - val_acc: 0.8559\n",
      "Epoch 123/300\n",
      "1058/1058 [==============================] - 0s 155us/step - loss: 0.3609 - acc: 0.8790 - val_loss: 0.3863 - val_acc: 0.8559\n",
      "Epoch 124/300\n",
      "1058/1058 [==============================] - 0s 151us/step - loss: 0.3454 - acc: 0.8856 - val_loss: 0.3945 - val_acc: 0.8644\n",
      "Epoch 125/300\n",
      "1058/1058 [==============================] - 0s 154us/step - loss: 0.3565 - acc: 0.8790 - val_loss: 0.3845 - val_acc: 0.8559\n",
      "Epoch 126/300\n",
      "1058/1058 [==============================] - 0s 153us/step - loss: 0.3486 - acc: 0.8875 - val_loss: 0.3818 - val_acc: 0.8559\n",
      "Epoch 127/300\n",
      "1058/1058 [==============================] - 0s 156us/step - loss: 0.3374 - acc: 0.8847 - val_loss: 0.3893 - val_acc: 0.8475\n",
      "Epoch 128/300\n",
      "1058/1058 [==============================] - 0s 154us/step - loss: 0.3481 - acc: 0.8819 - val_loss: 0.3829 - val_acc: 0.8559\n",
      "Epoch 129/300\n",
      "1058/1058 [==============================] - 0s 153us/step - loss: 0.3480 - acc: 0.8819 - val_loss: 0.3800 - val_acc: 0.8559\n",
      "Epoch 130/300\n",
      "1058/1058 [==============================] - 0s 153us/step - loss: 0.3510 - acc: 0.8847 - val_loss: 0.3913 - val_acc: 0.8729\n",
      "Epoch 131/300\n",
      "1058/1058 [==============================] - 0s 153us/step - loss: 0.3436 - acc: 0.8781 - val_loss: 0.3837 - val_acc: 0.8644\n",
      "Epoch 132/300\n",
      "1058/1058 [==============================] - 0s 156us/step - loss: 0.3190 - acc: 0.8894 - val_loss: 0.3870 - val_acc: 0.8475\n",
      "Epoch 133/300\n",
      "1058/1058 [==============================] - 0s 155us/step - loss: 0.3527 - acc: 0.8771 - val_loss: 0.3783 - val_acc: 0.8559\n",
      "Epoch 134/300\n",
      "1058/1058 [==============================] - 0s 155us/step - loss: 0.3672 - acc: 0.8715 - val_loss: 0.3826 - val_acc: 0.8729\n",
      "Epoch 135/300\n",
      "1058/1058 [==============================] - 0s 156us/step - loss: 0.3568 - acc: 0.8790 - val_loss: 0.3724 - val_acc: 0.8559\n",
      "Epoch 136/300\n",
      "1058/1058 [==============================] - 0s 155us/step - loss: 0.3375 - acc: 0.8856 - val_loss: 0.3775 - val_acc: 0.8475\n",
      "Epoch 137/300\n",
      "1058/1058 [==============================] - 0s 155us/step - loss: 0.3454 - acc: 0.8894 - val_loss: 0.3829 - val_acc: 0.8475\n",
      "Epoch 138/300\n",
      "1058/1058 [==============================] - 0s 162us/step - loss: 0.3449 - acc: 0.8828 - val_loss: 0.3777 - val_acc: 0.8475\n",
      "Epoch 139/300\n",
      "1058/1058 [==============================] - 0s 167us/step - loss: 0.3604 - acc: 0.8686 - val_loss: 0.3769 - val_acc: 0.8729\n",
      "Epoch 140/300\n",
      "1058/1058 [==============================] - 0s 168us/step - loss: 0.3432 - acc: 0.8894 - val_loss: 0.3768 - val_acc: 0.8729\n",
      "Epoch 141/300\n",
      "1058/1058 [==============================] - 0s 160us/step - loss: 0.3393 - acc: 0.8866 - val_loss: 0.3825 - val_acc: 0.8814\n",
      "Epoch 142/300\n",
      "1058/1058 [==============================] - 0s 156us/step - loss: 0.3601 - acc: 0.8781 - val_loss: 0.3788 - val_acc: 0.8644\n",
      "Epoch 143/300\n",
      "1058/1058 [==============================] - 0s 157us/step - loss: 0.3512 - acc: 0.8856 - val_loss: 0.3878 - val_acc: 0.8475\n",
      "Epoch 144/300\n",
      "1058/1058 [==============================] - 0s 205us/step - loss: 0.3286 - acc: 0.8875 - val_loss: 0.3848 - val_acc: 0.8644\n",
      "Epoch 145/300\n",
      "1058/1058 [==============================] - 0s 174us/step - loss: 0.3460 - acc: 0.8894 - val_loss: 0.3793 - val_acc: 0.8559\n",
      "Epoch 146/300\n",
      "1058/1058 [==============================] - 0s 171us/step - loss: 0.3453 - acc: 0.8847 - val_loss: 0.3919 - val_acc: 0.8898\n",
      "Epoch 147/300\n",
      "1058/1058 [==============================] - 0s 167us/step - loss: 0.3385 - acc: 0.8856 - val_loss: 0.3908 - val_acc: 0.8390\n",
      "Epoch 148/300\n",
      "1058/1058 [==============================] - 0s 159us/step - loss: 0.3514 - acc: 0.8866 - val_loss: 0.3840 - val_acc: 0.8644\n",
      "Epoch 149/300\n",
      "1058/1058 [==============================] - 0s 164us/step - loss: 0.3417 - acc: 0.8819 - val_loss: 0.3826 - val_acc: 0.8390\n",
      "Epoch 150/300\n",
      "1058/1058 [==============================] - 0s 156us/step - loss: 0.3339 - acc: 0.8856 - val_loss: 0.3838 - val_acc: 0.8390\n",
      "Epoch 151/300\n",
      "1058/1058 [==============================] - 0s 155us/step - loss: 0.3459 - acc: 0.8847 - val_loss: 0.3806 - val_acc: 0.8729\n",
      "Epoch 152/300\n",
      "1058/1058 [==============================] - 0s 156us/step - loss: 0.3590 - acc: 0.8715 - val_loss: 0.3771 - val_acc: 0.8814\n",
      "Epoch 153/300\n",
      "1058/1058 [==============================] - 0s 155us/step - loss: 0.3486 - acc: 0.8752 - val_loss: 0.3776 - val_acc: 0.8644\n",
      "Epoch 154/300\n",
      "1058/1058 [==============================] - 0s 155us/step - loss: 0.3452 - acc: 0.8790 - val_loss: 0.3779 - val_acc: 0.8644\n",
      "Epoch 155/300\n",
      "1058/1058 [==============================] - 0s 155us/step - loss: 0.3520 - acc: 0.8762 - val_loss: 0.3980 - val_acc: 0.8559\n",
      "Epoch 156/300\n",
      "1058/1058 [==============================] - 0s 156us/step - loss: 0.3541 - acc: 0.8837 - val_loss: 0.3790 - val_acc: 0.8644\n",
      "Epoch 157/300\n",
      "1058/1058 [==============================] - 0s 154us/step - loss: 0.3422 - acc: 0.8837 - val_loss: 0.4027 - val_acc: 0.8475\n",
      "Epoch 158/300\n",
      "1058/1058 [==============================] - 0s 155us/step - loss: 0.3516 - acc: 0.8743 - val_loss: 0.3794 - val_acc: 0.8644\n",
      "Epoch 159/300\n",
      "1058/1058 [==============================] - 0s 156us/step - loss: 0.3189 - acc: 0.8894 - val_loss: 0.3891 - val_acc: 0.8305\n",
      "Epoch 160/300\n",
      "1058/1058 [==============================] - 0s 158us/step - loss: 0.3479 - acc: 0.8866 - val_loss: 0.3766 - val_acc: 0.8729\n",
      "Epoch 161/300\n",
      "1058/1058 [==============================] - 0s 160us/step - loss: 0.3317 - acc: 0.8894 - val_loss: 0.3925 - val_acc: 0.8305\n",
      "Epoch 162/300\n",
      "1058/1058 [==============================] - 0s 163us/step - loss: 0.3412 - acc: 0.8837 - val_loss: 0.3931 - val_acc: 0.8729\n",
      "Epoch 163/300\n",
      "1058/1058 [==============================] - 0s 159us/step - loss: 0.3385 - acc: 0.8856 - val_loss: 0.3811 - val_acc: 0.8390\n",
      "Epoch 164/300\n",
      "1058/1058 [==============================] - 0s 155us/step - loss: 0.3264 - acc: 0.8904 - val_loss: 0.3888 - val_acc: 0.8390\n",
      "Epoch 165/300\n",
      "1058/1058 [==============================] - 0s 156us/step - loss: 0.3242 - acc: 0.8904 - val_loss: 0.3793 - val_acc: 0.8644\n",
      "Epoch 166/300\n",
      "1058/1058 [==============================] - 0s 154us/step - loss: 0.3398 - acc: 0.8819 - val_loss: 0.3882 - val_acc: 0.8729\n",
      "Epoch 167/300\n",
      "1058/1058 [==============================] - 0s 155us/step - loss: 0.3343 - acc: 0.8970 - val_loss: 0.3885 - val_acc: 0.8305\n",
      "Epoch 168/300\n",
      "1058/1058 [==============================] - 0s 155us/step - loss: 0.3285 - acc: 0.8932 - val_loss: 0.3791 - val_acc: 0.8644\n",
      "Epoch 169/300\n",
      "1058/1058 [==============================] - 0s 154us/step - loss: 0.3235 - acc: 0.8866 - val_loss: 0.3772 - val_acc: 0.8644\n",
      "Epoch 170/300\n",
      "1058/1058 [==============================] - 0s 155us/step - loss: 0.3218 - acc: 0.8885 - val_loss: 0.3755 - val_acc: 0.8559\n",
      "Epoch 171/300\n",
      "1058/1058 [==============================] - 0s 166us/step - loss: 0.3228 - acc: 0.8951 - val_loss: 0.3928 - val_acc: 0.8305\n",
      "Epoch 172/300\n",
      "1058/1058 [==============================] - 0s 186us/step - loss: 0.3213 - acc: 0.8951 - val_loss: 0.3857 - val_acc: 0.8390\n",
      "Epoch 173/300\n",
      "1058/1058 [==============================] - 0s 169us/step - loss: 0.3398 - acc: 0.8856 - val_loss: 0.3866 - val_acc: 0.8475\n",
      "Epoch 174/300\n",
      "1058/1058 [==============================] - 0s 157us/step - loss: 0.3256 - acc: 0.8856 - val_loss: 0.3779 - val_acc: 0.8729\n",
      "Epoch 175/300\n",
      "1058/1058 [==============================] - 0s 175us/step - loss: 0.3348 - acc: 0.8771 - val_loss: 0.3749 - val_acc: 0.8729\n",
      "Epoch 176/300\n",
      "1058/1058 [==============================] - 0s 180us/step - loss: 0.3319 - acc: 0.8809 - val_loss: 0.3788 - val_acc: 0.8559\n",
      "Epoch 177/300\n",
      "1058/1058 [==============================] - 0s 164us/step - loss: 0.3255 - acc: 0.8885 - val_loss: 0.3835 - val_acc: 0.8475\n",
      "Epoch 178/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1058/1058 [==============================] - 0s 157us/step - loss: 0.3278 - acc: 0.8894 - val_loss: 0.3756 - val_acc: 0.8644\n",
      "Epoch 179/300\n",
      "1058/1058 [==============================] - 0s 157us/step - loss: 0.3347 - acc: 0.8885 - val_loss: 0.3839 - val_acc: 0.8814\n",
      "Epoch 180/300\n",
      "1058/1058 [==============================] - 0s 163us/step - loss: 0.3348 - acc: 0.8866 - val_loss: 0.3926 - val_acc: 0.8305\n",
      "Epoch 181/300\n",
      "1058/1058 [==============================] - 0s 154us/step - loss: 0.3361 - acc: 0.8904 - val_loss: 0.3852 - val_acc: 0.8475\n",
      "Epoch 182/300\n",
      "1058/1058 [==============================] - 0s 155us/step - loss: 0.3273 - acc: 0.8913 - val_loss: 0.3775 - val_acc: 0.8644\n",
      "Epoch 183/300\n",
      "1058/1058 [==============================] - 0s 156us/step - loss: 0.3259 - acc: 0.8828 - val_loss: 0.3918 - val_acc: 0.8390\n",
      "Epoch 184/300\n",
      "1058/1058 [==============================] - 0s 155us/step - loss: 0.3227 - acc: 0.8894 - val_loss: 0.3853 - val_acc: 0.8475\n",
      "Epoch 185/300\n",
      "1058/1058 [==============================] - 0s 155us/step - loss: 0.3214 - acc: 0.8922 - val_loss: 0.3855 - val_acc: 0.8475\n",
      "\n",
      "Epoch 00185: ReduceLROnPlateau reducing learning rate to 0.0019999999553.\n",
      "Epoch 186/300\n",
      "1058/1058 [==============================] - 0s 163us/step - loss: 0.3539 - acc: 0.8800 - val_loss: 0.3863 - val_acc: 0.8559\n",
      "Epoch 187/300\n",
      "1058/1058 [==============================] - 0s 155us/step - loss: 0.3334 - acc: 0.8847 - val_loss: 0.3823 - val_acc: 0.8559\n",
      "Epoch 188/300\n",
      "1058/1058 [==============================] - 0s 155us/step - loss: 0.3363 - acc: 0.8913 - val_loss: 0.3824 - val_acc: 0.8559\n",
      "Epoch 189/300\n",
      "1058/1058 [==============================] - 0s 161us/step - loss: 0.3277 - acc: 0.8922 - val_loss: 0.3863 - val_acc: 0.8559\n",
      "Epoch 190/300\n",
      "1058/1058 [==============================] - 0s 162us/step - loss: 0.3200 - acc: 0.8894 - val_loss: 0.3870 - val_acc: 0.8644\n",
      "Epoch 191/300\n",
      "1058/1058 [==============================] - 0s 156us/step - loss: 0.3252 - acc: 0.8951 - val_loss: 0.3879 - val_acc: 0.8644\n",
      "Epoch 192/300\n",
      "1058/1058 [==============================] - 0s 156us/step - loss: 0.3156 - acc: 0.8932 - val_loss: 0.3877 - val_acc: 0.8644\n",
      "Epoch 193/300\n",
      "1058/1058 [==============================] - 0s 156us/step - loss: 0.3223 - acc: 0.8875 - val_loss: 0.3867 - val_acc: 0.8644\n",
      "Epoch 194/300\n",
      "1058/1058 [==============================] - 0s 157us/step - loss: 0.3084 - acc: 0.8998 - val_loss: 0.3897 - val_acc: 0.8559\n",
      "Epoch 195/300\n",
      "1058/1058 [==============================] - 0s 157us/step - loss: 0.3257 - acc: 0.8866 - val_loss: 0.3890 - val_acc: 0.8644\n",
      "Epoch 196/300\n",
      "1058/1058 [==============================] - 0s 155us/step - loss: 0.3161 - acc: 0.8960 - val_loss: 0.3859 - val_acc: 0.8644\n",
      "Epoch 197/300\n",
      "1058/1058 [==============================] - 0s 155us/step - loss: 0.3216 - acc: 0.8913 - val_loss: 0.3842 - val_acc: 0.8644\n",
      "Epoch 198/300\n",
      "1058/1058 [==============================] - 0s 155us/step - loss: 0.3211 - acc: 0.8885 - val_loss: 0.3820 - val_acc: 0.8644\n",
      "Epoch 199/300\n",
      "1058/1058 [==============================] - 0s 157us/step - loss: 0.3216 - acc: 0.8913 - val_loss: 0.3845 - val_acc: 0.8644\n",
      "Epoch 200/300\n",
      "1058/1058 [==============================] - 0s 162us/step - loss: 0.3137 - acc: 0.8932 - val_loss: 0.3864 - val_acc: 0.8644\n",
      "Epoch 201/300\n",
      "1058/1058 [==============================] - 0s 159us/step - loss: 0.3171 - acc: 0.8856 - val_loss: 0.3826 - val_acc: 0.8644\n",
      "Epoch 202/300\n",
      "1058/1058 [==============================] - 0s 157us/step - loss: 0.3075 - acc: 0.8970 - val_loss: 0.3838 - val_acc: 0.8644\n",
      "Epoch 203/300\n",
      "1058/1058 [==============================] - 0s 158us/step - loss: 0.3190 - acc: 0.8866 - val_loss: 0.3835 - val_acc: 0.8644\n",
      "Epoch 204/300\n",
      "1058/1058 [==============================] - 0s 158us/step - loss: 0.3088 - acc: 0.8941 - val_loss: 0.3877 - val_acc: 0.8475\n",
      "Epoch 205/300\n",
      "1058/1058 [==============================] - 0s 156us/step - loss: 0.3143 - acc: 0.8941 - val_loss: 0.3830 - val_acc: 0.8644\n",
      "Epoch 206/300\n",
      "1058/1058 [==============================] - 0s 160us/step - loss: 0.3257 - acc: 0.8913 - val_loss: 0.3821 - val_acc: 0.8644\n",
      "Epoch 207/300\n",
      "1058/1058 [==============================] - 0s 158us/step - loss: 0.3092 - acc: 0.8913 - val_loss: 0.3872 - val_acc: 0.8559\n",
      "Epoch 208/300\n",
      "1058/1058 [==============================] - 0s 163us/step - loss: 0.3243 - acc: 0.8951 - val_loss: 0.3853 - val_acc: 0.8644\n",
      "Epoch 209/300\n",
      "1058/1058 [==============================] - 0s 165us/step - loss: 0.3176 - acc: 0.8989 - val_loss: 0.3836 - val_acc: 0.8644\n",
      "Epoch 210/300\n",
      "1058/1058 [==============================] - 0s 157us/step - loss: 0.3143 - acc: 0.8951 - val_loss: 0.3841 - val_acc: 0.8644\n",
      "Epoch 211/300\n",
      "1058/1058 [==============================] - 0s 159us/step - loss: 0.3016 - acc: 0.8970 - val_loss: 0.3872 - val_acc: 0.8559\n",
      "Epoch 212/300\n",
      "1058/1058 [==============================] - 0s 158us/step - loss: 0.3061 - acc: 0.8998 - val_loss: 0.3892 - val_acc: 0.8475\n",
      "Epoch 213/300\n",
      "1058/1058 [==============================] - 0s 162us/step - loss: 0.3029 - acc: 0.9008 - val_loss: 0.3847 - val_acc: 0.8559\n",
      "Epoch 214/300\n",
      "1058/1058 [==============================] - 0s 159us/step - loss: 0.3092 - acc: 0.8960 - val_loss: 0.3854 - val_acc: 0.8559\n",
      "Epoch 215/300\n",
      "1058/1058 [==============================] - 0s 159us/step - loss: 0.3272 - acc: 0.8904 - val_loss: 0.3852 - val_acc: 0.8475\n",
      "Epoch 216/300\n",
      "1058/1058 [==============================] - 0s 164us/step - loss: 0.3179 - acc: 0.8913 - val_loss: 0.3821 - val_acc: 0.8559\n",
      "Epoch 217/300\n",
      "1058/1058 [==============================] - 0s 159us/step - loss: 0.3080 - acc: 0.8951 - val_loss: 0.3817 - val_acc: 0.8559\n",
      "Epoch 218/300\n",
      "1058/1058 [==============================] - 0s 163us/step - loss: 0.3034 - acc: 0.8970 - val_loss: 0.3822 - val_acc: 0.8559\n",
      "Epoch 219/300\n",
      "1058/1058 [==============================] - 0s 164us/step - loss: 0.3127 - acc: 0.8932 - val_loss: 0.3811 - val_acc: 0.8644\n",
      "Epoch 220/300\n",
      "1058/1058 [==============================] - 0s 160us/step - loss: 0.2998 - acc: 0.8941 - val_loss: 0.3815 - val_acc: 0.8644\n",
      "Epoch 221/300\n",
      "1058/1058 [==============================] - 0s 157us/step - loss: 0.3022 - acc: 0.8951 - val_loss: 0.3833 - val_acc: 0.8559\n",
      "Epoch 222/300\n",
      "1058/1058 [==============================] - 0s 159us/step - loss: 0.3069 - acc: 0.8979 - val_loss: 0.3799 - val_acc: 0.8559\n",
      "Epoch 223/300\n",
      "1058/1058 [==============================] - 0s 163us/step - loss: 0.3134 - acc: 0.8885 - val_loss: 0.3801 - val_acc: 0.8559\n",
      "Epoch 224/300\n",
      "1058/1058 [==============================] - 0s 163us/step - loss: 0.3080 - acc: 0.8989 - val_loss: 0.3827 - val_acc: 0.8559\n",
      "Epoch 225/300\n",
      "1058/1058 [==============================] - 0s 158us/step - loss: 0.3178 - acc: 0.8922 - val_loss: 0.3799 - val_acc: 0.8559\n",
      "Epoch 226/300\n",
      "1058/1058 [==============================] - 0s 163us/step - loss: 0.3051 - acc: 0.8960 - val_loss: 0.3838 - val_acc: 0.8559\n",
      "Epoch 227/300\n",
      "1058/1058 [==============================] - 0s 160us/step - loss: 0.3081 - acc: 0.8922 - val_loss: 0.3822 - val_acc: 0.8559\n",
      "Epoch 228/300\n",
      "1058/1058 [==============================] - 0s 163us/step - loss: 0.3101 - acc: 0.8970 - val_loss: 0.3827 - val_acc: 0.8559\n",
      "Epoch 229/300\n",
      "1058/1058 [==============================] - 0s 160us/step - loss: 0.3117 - acc: 0.8913 - val_loss: 0.3826 - val_acc: 0.8390\n",
      "Epoch 230/300\n",
      "1058/1058 [==============================] - 0s 157us/step - loss: 0.3066 - acc: 0.9036 - val_loss: 0.3784 - val_acc: 0.8559\n",
      "Epoch 231/300\n",
      "1058/1058 [==============================] - 0s 165us/step - loss: 0.2975 - acc: 0.9008 - val_loss: 0.3801 - val_acc: 0.8559\n",
      "Epoch 232/300\n",
      "1058/1058 [==============================] - 0s 165us/step - loss: 0.3112 - acc: 0.8951 - val_loss: 0.3770 - val_acc: 0.8644\n",
      "Epoch 233/300\n",
      "1058/1058 [==============================] - 0s 158us/step - loss: 0.3107 - acc: 0.8960 - val_loss: 0.3711 - val_acc: 0.8644\n",
      "Epoch 234/300\n",
      "1058/1058 [==============================] - 0s 157us/step - loss: 0.3285 - acc: 0.8800 - val_loss: 0.3772 - val_acc: 0.8644\n",
      "Epoch 235/300\n",
      "1058/1058 [==============================] - 0s 167us/step - loss: 0.3247 - acc: 0.8885 - val_loss: 0.3648 - val_acc: 0.8814\n",
      "Epoch 236/300\n",
      "1058/1058 [==============================] - 0s 161us/step - loss: 0.3133 - acc: 0.8979 - val_loss: 0.3672 - val_acc: 0.8814\n",
      "Epoch 237/300\n",
      "1058/1058 [==============================] - 0s 163us/step - loss: 0.3225 - acc: 0.8866 - val_loss: 0.3668 - val_acc: 0.8814\n",
      "Epoch 238/300\n",
      "1058/1058 [==============================] - 0s 158us/step - loss: 0.3080 - acc: 0.8904 - val_loss: 0.3677 - val_acc: 0.8644\n",
      "Epoch 239/300\n",
      "1058/1058 [==============================] - 0s 158us/step - loss: 0.3375 - acc: 0.8932 - val_loss: 0.3688 - val_acc: 0.8644\n",
      "Epoch 240/300\n",
      "1058/1058 [==============================] - 0s 161us/step - loss: 0.3167 - acc: 0.8979 - val_loss: 0.3690 - val_acc: 0.8644\n",
      "Epoch 241/300\n",
      "1058/1058 [==============================] - 0s 164us/step - loss: 0.3070 - acc: 0.8894 - val_loss: 0.3720 - val_acc: 0.8644\n",
      "Epoch 242/300\n",
      "1058/1058 [==============================] - 0s 158us/step - loss: 0.3062 - acc: 0.8922 - val_loss: 0.3712 - val_acc: 0.8644\n",
      "Epoch 243/300\n",
      "1058/1058 [==============================] - 0s 161us/step - loss: 0.3205 - acc: 0.8913 - val_loss: 0.3699 - val_acc: 0.8644\n",
      "Epoch 244/300\n",
      "1058/1058 [==============================] - 0s 163us/step - loss: 0.3110 - acc: 0.8875 - val_loss: 0.3706 - val_acc: 0.8644\n",
      "Epoch 245/300\n",
      "1058/1058 [==============================] - 0s 157us/step - loss: 0.3097 - acc: 0.9008 - val_loss: 0.3736 - val_acc: 0.8644\n",
      "Epoch 246/300\n",
      "1058/1058 [==============================] - 0s 164us/step - loss: 0.3131 - acc: 0.8932 - val_loss: 0.3702 - val_acc: 0.8644\n",
      "Epoch 247/300\n",
      "1058/1058 [==============================] - 0s 159us/step - loss: 0.3107 - acc: 0.8970 - val_loss: 0.3709 - val_acc: 0.8644\n",
      "Epoch 248/300\n",
      "1058/1058 [==============================] - 0s 158us/step - loss: 0.3015 - acc: 0.8960 - val_loss: 0.3719 - val_acc: 0.8644\n",
      "Epoch 249/300\n",
      "1058/1058 [==============================] - 0s 158us/step - loss: 0.3056 - acc: 0.8970 - val_loss: 0.3717 - val_acc: 0.8644\n",
      "Epoch 250/300\n",
      "1058/1058 [==============================] - 0s 158us/step - loss: 0.3062 - acc: 0.8922 - val_loss: 0.3716 - val_acc: 0.8644\n",
      "Epoch 251/300\n",
      "1058/1058 [==============================] - 0s 158us/step - loss: 0.3126 - acc: 0.8960 - val_loss: 0.3685 - val_acc: 0.8644\n",
      "Epoch 252/300\n",
      "1058/1058 [==============================] - 0s 158us/step - loss: 0.3153 - acc: 0.8998 - val_loss: 0.3673 - val_acc: 0.8644\n",
      "Epoch 253/300\n",
      "1058/1058 [==============================] - 0s 163us/step - loss: 0.3000 - acc: 0.8951 - val_loss: 0.3697 - val_acc: 0.8644\n",
      "Epoch 254/300\n",
      "1058/1058 [==============================] - 0s 159us/step - loss: 0.3077 - acc: 0.8932 - val_loss: 0.3662 - val_acc: 0.8644\n",
      "Epoch 255/300\n",
      "1058/1058 [==============================] - 0s 164us/step - loss: 0.3040 - acc: 0.8989 - val_loss: 0.3684 - val_acc: 0.8644\n",
      "Epoch 256/300\n",
      "1058/1058 [==============================] - 0s 158us/step - loss: 0.3122 - acc: 0.8856 - val_loss: 0.3655 - val_acc: 0.8644\n",
      "Epoch 257/300\n",
      "1058/1058 [==============================] - 0s 159us/step - loss: 0.3066 - acc: 0.8998 - val_loss: 0.3684 - val_acc: 0.8644\n",
      "Epoch 258/300\n",
      "1058/1058 [==============================] - 0s 159us/step - loss: 0.3123 - acc: 0.8904 - val_loss: 0.3674 - val_acc: 0.8644\n",
      "Epoch 259/300\n",
      "1058/1058 [==============================] - 0s 164us/step - loss: 0.3124 - acc: 0.8951 - val_loss: 0.3712 - val_acc: 0.8644\n",
      "Epoch 260/300\n",
      "1058/1058 [==============================] - 0s 164us/step - loss: 0.3074 - acc: 0.8922 - val_loss: 0.3733 - val_acc: 0.8559\n",
      "Epoch 261/300\n",
      "1058/1058 [==============================] - 0s 158us/step - loss: 0.3065 - acc: 0.8970 - val_loss: 0.3702 - val_acc: 0.8644\n",
      "Epoch 262/300\n",
      "1058/1058 [==============================] - 0s 162us/step - loss: 0.2916 - acc: 0.9036 - val_loss: 0.3697 - val_acc: 0.8644\n",
      "Epoch 263/300\n",
      "1058/1058 [==============================] - 0s 160us/step - loss: 0.3145 - acc: 0.8904 - val_loss: 0.3588 - val_acc: 0.8644\n",
      "Epoch 264/300\n",
      "1058/1058 [==============================] - 0s 161us/step - loss: 0.3170 - acc: 0.8941 - val_loss: 0.3606 - val_acc: 0.8644\n",
      "Epoch 265/300\n",
      "1058/1058 [==============================] - 0s 157us/step - loss: 0.3128 - acc: 0.8894 - val_loss: 0.3619 - val_acc: 0.8644\n",
      "Epoch 266/300\n",
      "1058/1058 [==============================] - 0s 159us/step - loss: 0.3060 - acc: 0.8922 - val_loss: 0.3645 - val_acc: 0.8644\n",
      "Epoch 267/300\n",
      "1058/1058 [==============================] - 0s 157us/step - loss: 0.3163 - acc: 0.8904 - val_loss: 0.3655 - val_acc: 0.8644\n",
      "Epoch 268/300\n",
      "1058/1058 [==============================] - 0s 158us/step - loss: 0.3086 - acc: 0.8979 - val_loss: 0.3648 - val_acc: 0.8644\n",
      "Epoch 269/300\n",
      "1058/1058 [==============================] - 0s 161us/step - loss: 0.3049 - acc: 0.9026 - val_loss: 0.3666 - val_acc: 0.8644\n",
      "Epoch 270/300\n",
      "1058/1058 [==============================] - 0s 162us/step - loss: 0.3069 - acc: 0.8998 - val_loss: 0.3690 - val_acc: 0.8644\n",
      "Epoch 271/300\n",
      "1058/1058 [==============================] - 0s 163us/step - loss: 0.3085 - acc: 0.8970 - val_loss: 0.3657 - val_acc: 0.8644\n",
      "Epoch 272/300\n",
      "1058/1058 [==============================] - 0s 159us/step - loss: 0.3064 - acc: 0.8941 - val_loss: 0.3667 - val_acc: 0.8644\n",
      "Epoch 273/300\n",
      "1058/1058 [==============================] - 0s 158us/step - loss: 0.3126 - acc: 0.8904 - val_loss: 0.3666 - val_acc: 0.8644\n",
      "Epoch 274/300\n",
      "1058/1058 [==============================] - 0s 162us/step - loss: 0.3131 - acc: 0.8922 - val_loss: 0.3623 - val_acc: 0.8814\n",
      "Epoch 275/300\n",
      "1058/1058 [==============================] - 0s 157us/step - loss: 0.3005 - acc: 0.9017 - val_loss: 0.3623 - val_acc: 0.8644\n",
      "Epoch 276/300\n",
      "1058/1058 [==============================] - 0s 159us/step - loss: 0.3097 - acc: 0.8913 - val_loss: 0.3622 - val_acc: 0.8644\n",
      "Epoch 277/300\n",
      "1058/1058 [==============================] - 0s 166us/step - loss: 0.3028 - acc: 0.8979 - val_loss: 0.3613 - val_acc: 0.8644\n",
      "Epoch 278/300\n",
      "1058/1058 [==============================] - 0s 162us/step - loss: 0.3104 - acc: 0.8998 - val_loss: 0.3582 - val_acc: 0.8644\n",
      "Epoch 279/300\n",
      "1058/1058 [==============================] - 0s 156us/step - loss: 0.3194 - acc: 0.8989 - val_loss: 0.3604 - val_acc: 0.8644\n",
      "Epoch 280/300\n",
      "1058/1058 [==============================] - 0s 158us/step - loss: 0.3137 - acc: 0.8960 - val_loss: 0.3588 - val_acc: 0.8644\n",
      "Epoch 281/300\n",
      "1058/1058 [==============================] - 0s 159us/step - loss: 0.3108 - acc: 0.8951 - val_loss: 0.3618 - val_acc: 0.8644\n",
      "Epoch 282/300\n",
      "1058/1058 [==============================] - 0s 158us/step - loss: 0.3101 - acc: 0.8932 - val_loss: 0.3635 - val_acc: 0.8644\n",
      "Epoch 283/300\n",
      "1058/1058 [==============================] - 0s 164us/step - loss: 0.3037 - acc: 0.8989 - val_loss: 0.3644 - val_acc: 0.8644\n",
      "Epoch 284/300\n",
      "1058/1058 [==============================] - 0s 162us/step - loss: 0.2999 - acc: 0.8970 - val_loss: 0.3636 - val_acc: 0.8644\n",
      "Epoch 285/300\n",
      "1058/1058 [==============================] - 0s 162us/step - loss: 0.3024 - acc: 0.8951 - val_loss: 0.3642 - val_acc: 0.8644\n",
      "Epoch 286/300\n",
      "1058/1058 [==============================] - 0s 164us/step - loss: 0.3087 - acc: 0.8989 - val_loss: 0.3627 - val_acc: 0.8644\n",
      "Epoch 287/300\n",
      "1058/1058 [==============================] - 0s 158us/step - loss: 0.2957 - acc: 0.8998 - val_loss: 0.3655 - val_acc: 0.8644\n",
      "Epoch 288/300\n",
      "1058/1058 [==============================] - 0s 159us/step - loss: 0.3058 - acc: 0.8932 - val_loss: 0.3650 - val_acc: 0.8644\n",
      "Epoch 289/300\n",
      "1058/1058 [==============================] - 0s 164us/step - loss: 0.3043 - acc: 0.8998 - val_loss: 0.3650 - val_acc: 0.8644\n",
      "Epoch 290/300\n",
      "1058/1058 [==============================] - 0s 160us/step - loss: 0.2974 - acc: 0.8932 - val_loss: 0.3628 - val_acc: 0.8644\n",
      "Epoch 291/300\n",
      "1058/1058 [==============================] - 0s 163us/step - loss: 0.3058 - acc: 0.8951 - val_loss: 0.3614 - val_acc: 0.8644\n",
      "Epoch 292/300\n",
      "1058/1058 [==============================] - 0s 159us/step - loss: 0.3019 - acc: 0.8913 - val_loss: 0.3667 - val_acc: 0.8644\n",
      "Epoch 293/300\n",
      "1058/1058 [==============================] - 0s 158us/step - loss: 0.2932 - acc: 0.9017 - val_loss: 0.3693 - val_acc: 0.8644\n",
      "Epoch 294/300\n",
      "1058/1058 [==============================] - 0s 157us/step - loss: 0.3079 - acc: 0.8960 - val_loss: 0.3697 - val_acc: 0.8644\n",
      "Epoch 295/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1058/1058 [==============================] - 0s 160us/step - loss: 0.3047 - acc: 0.8913 - val_loss: 0.3683 - val_acc: 0.8644\n",
      "Epoch 296/300\n",
      "1058/1058 [==============================] - 0s 155us/step - loss: 0.3018 - acc: 0.9055 - val_loss: 0.3666 - val_acc: 0.8644\n",
      "Epoch 297/300\n",
      "1058/1058 [==============================] - 0s 158us/step - loss: 0.2939 - acc: 0.9036 - val_loss: 0.3664 - val_acc: 0.8559\n",
      "Epoch 298/300\n",
      "1058/1058 [==============================] - 0s 157us/step - loss: 0.3184 - acc: 0.8960 - val_loss: 0.3658 - val_acc: 0.8644\n",
      "Epoch 299/300\n",
      "1058/1058 [==============================] - 0s 159us/step - loss: 0.3175 - acc: 0.8885 - val_loss: 0.3678 - val_acc: 0.8644\n",
      "Epoch 300/300\n",
      "1058/1058 [==============================] - 0s 161us/step - loss: 0.3142 - acc: 0.8922 - val_loss: 0.3620 - val_acc: 0.8559\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, batch_size = 32, epochs = 300,\n",
    "                    validation_split = 0.1, verbose = 1, callbacks = [checkpoint, reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##0 no zero database ~%88.4\n",
    "##1 no zero db, .5 Dropout, l2 ~%88.78\n",
    "##2 zero database ~%88.5\n",
    "\n",
    "model.load_weights('weights2.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "294/294 [==============================] - 0s 51us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.37854802664242632, 0.88775510204081631]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 735,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 85.81%\n",
      "acc: 89.86%\n",
      "acc: 89.19%\n",
      "acc: 86.39%\n",
      "acc: 87.07%\n",
      "acc: 87.07%\n",
      "acc: 88.44%\n",
      "acc: 89.04%\n",
      "acc: 86.99%\n",
      "acc: 85.62%\n",
      "87.55% (+/- 1.41%)\n"
     ]
    }
   ],
   "source": [
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "num_classes = 1\n",
    "input_shape = dummy_scaled_df.shape[1]\n",
    "activations = 'tanh'\n",
    "X = np.array(all_scaled)\n",
    "y = np.array(labels)\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "cvscores = []\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "for train, test in kfold.split(X, y):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(input_shape, input_shape = (input_shape,), activation = activations,\n",
    "                kernel_regularizer = regularizers.l2(.001)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(64, activation = 'relu', kernel_regularizer = regularizers.l2(.001)))#1024\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(64, activation = 'relu', kernel_regularizer = regularizers.l2(.001)))#1024\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation = 'sigmoid'))\n",
    "    \n",
    "    cv_checkpoint = ModelCheckpoint(filepath = 'cv_weights.hdf5', save_best_only = True, monitor='val_loss') \n",
    "    model.compile(loss = 'binary_crossentropy', optimizer = SGD(lr=0.0001, momentum = 0.9), metrics = ['accuracy'])\n",
    "    model.fit(X[train], y[train], epochs = 3000, batch_size = 32, verbose = 0,\n",
    "              callbacks = [cv_checkpoint], validation_split = 0.1)\n",
    "    \n",
    "    model.load_weights('cv_weights.hdf5')\n",
    "    scores = model.evaluate(X[test], y[test], verbose=0)\n",
    "    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "    cvscores.append(scores[1] * 100)\n",
    "print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, (array([ 0.12688495], dtype=float32), 0))\n",
      "(1, (array([ 0.02242634], dtype=float32), 0))\n",
      "(2, (array([ 0.17138575], dtype=float32), 1))\n",
      "(3, (array([ 0.02340447], dtype=float32), 0))\n",
      "(4, (array([ 0.07922949], dtype=float32), 0))\n",
      "(5, (array([ 0.13620339], dtype=float32), 0))\n",
      "(6, (array([ 0.29969111], dtype=float32), 0))\n",
      "(7, (array([ 0.03151904], dtype=float32), 0))\n",
      "(8, (array([ 0.07154495], dtype=float32), 0))\n",
      "(9, (array([ 0.01828349], dtype=float32), 0))\n",
      "(10, (array([ 0.61181146], dtype=float32), 0))\n",
      "(11, (array([ 0.03600796], dtype=float32), 0))\n",
      "(12, (array([ 0.03927429], dtype=float32), 0))\n",
      "(13, (array([ 0.05277799], dtype=float32), 0))\n",
      "(14, (array([ 0.02737933], dtype=float32), 0))\n",
      "(15, (array([ 0.23258872], dtype=float32), 0))\n",
      "(16, (array([ 0.03946596], dtype=float32), 0))\n",
      "(17, (array([ 0.1373646], dtype=float32), 1))\n",
      "(18, (array([ 0.24305572], dtype=float32), 1))\n",
      "(19, (array([ 0.02635566], dtype=float32), 0))\n",
      "(20, (array([ 0.82215291], dtype=float32), 1))\n",
      "(21, (array([ 0.01355755], dtype=float32), 0))\n",
      "(22, (array([ 0.01198612], dtype=float32), 0))\n",
      "(23, (array([ 0.32943743], dtype=float32), 0))\n",
      "(24, (array([ 0.12551028], dtype=float32), 0))\n",
      "(25, (array([ 0.00822452], dtype=float32), 0))\n",
      "(26, (array([ 0.02129954], dtype=float32), 0))\n",
      "(27, (array([ 0.01168826], dtype=float32), 0))\n",
      "(28, (array([ 0.06530289], dtype=float32), 0))\n",
      "(29, (array([ 0.05304299], dtype=float32), 0))\n",
      "(30, (array([ 0.00461093], dtype=float32), 0))\n",
      "(31, (array([ 0.02818433], dtype=float32), 0))\n",
      "(32, (array([ 0.00169199], dtype=float32), 0))\n",
      "(33, (array([ 0.12790167], dtype=float32), 0))\n",
      "(34, (array([ 0.9759807], dtype=float32), 1))\n",
      "(35, (array([ 0.00739822], dtype=float32), 0))\n",
      "(36, (array([ 0.00123514], dtype=float32), 0))\n",
      "(37, (array([ 0.01831967], dtype=float32), 1))\n",
      "(38, (array([ 0.88505703], dtype=float32), 1))\n",
      "(39, (array([ 0.00609114], dtype=float32), 0))\n",
      "(40, (array([ 0.01055907], dtype=float32), 0))\n",
      "(41, (array([ 0.15038168], dtype=float32), 0))\n",
      "(42, (array([ 0.074903], dtype=float32), 0))\n",
      "(43, (array([ 0.05997153], dtype=float32), 0))\n",
      "(44, (array([ 0.90954655], dtype=float32), 1))\n",
      "(45, (array([ 0.0086534], dtype=float32), 0))\n",
      "(46, (array([ 0.92034996], dtype=float32), 1))\n",
      "(47, (array([ 0.23831865], dtype=float32), 1))\n",
      "(48, (array([ 0.14345723], dtype=float32), 1))\n",
      "(49, (array([ 0.95687324], dtype=float32), 1))\n",
      "(50, (array([ 0.01016066], dtype=float32), 0))\n",
      "(51, (array([ 0.21093996], dtype=float32), 0))\n",
      "(52, (array([ 0.02670476], dtype=float32), 0))\n",
      "(53, (array([ 0.14664699], dtype=float32), 0))\n",
      "(54, (array([ 0.047565], dtype=float32), 0))\n",
      "(55, (array([ 0.06148623], dtype=float32), 0))\n",
      "(56, (array([ 0.28335884], dtype=float32), 0))\n",
      "(57, (array([ 0.00089493], dtype=float32), 0))\n",
      "(58, (array([ 0.03127646], dtype=float32), 0))\n",
      "(59, (array([ 0.14697582], dtype=float32), 0))\n",
      "(60, (array([ 0.00669989], dtype=float32), 0))\n",
      "(61, (array([ 0.33809584], dtype=float32), 1))\n",
      "(62, (array([ 0.12630762], dtype=float32), 1))\n",
      "(63, (array([ 0.00436387], dtype=float32), 0))\n",
      "(64, (array([ 0.04096768], dtype=float32), 1))\n",
      "(65, (array([ 0.24681175], dtype=float32), 1))\n",
      "(66, (array([ 0.0109914], dtype=float32), 0))\n",
      "(67, (array([ 0.04874759], dtype=float32), 0))\n",
      "(68, (array([ 0.00251012], dtype=float32), 0))\n",
      "(69, (array([ 0.11128888], dtype=float32), 0))\n",
      "(70, (array([ 0.05479385], dtype=float32), 0))\n",
      "(71, (array([ 0.11424841], dtype=float32), 0))\n",
      "(72, (array([ 0.2274821], dtype=float32), 0))\n",
      "(73, (array([ 0.072233], dtype=float32), 0))\n",
      "(74, (array([ 0.02115595], dtype=float32), 0))\n",
      "(75, (array([ 0.02391276], dtype=float32), 0))\n",
      "(76, (array([ 0.04515762], dtype=float32), 0))\n",
      "(77, (array([ 0.05390144], dtype=float32), 0))\n",
      "(78, (array([ 0.13580097], dtype=float32), 0))\n",
      "(79, (array([ 0.02719412], dtype=float32), 0))\n",
      "(80, (array([ 0.0030494], dtype=float32), 0))\n",
      "(81, (array([ 0.09684116], dtype=float32), 0))\n",
      "(82, (array([ 0.07024045], dtype=float32), 0))\n",
      "(83, (array([ 0.14152013], dtype=float32), 0))\n",
      "(84, (array([ 0.07645972], dtype=float32), 1))\n",
      "(85, (array([ 0.02612344], dtype=float32), 0))\n",
      "(86, (array([ 0.22892216], dtype=float32), 0))\n",
      "(87, (array([ 0.08885017], dtype=float32), 0))\n",
      "(88, (array([ 0.01928986], dtype=float32), 0))\n",
      "(89, (array([ 0.03091195], dtype=float32), 0))\n",
      "(90, (array([ 0.009705], dtype=float32), 0))\n",
      "(91, (array([ 0.02114795], dtype=float32), 0))\n",
      "(92, (array([ 0.19919866], dtype=float32), 0))\n",
      "(93, (array([ 0.1423637], dtype=float32), 0))\n",
      "(94, (array([ 0.10108241], dtype=float32), 0))\n",
      "(95, (array([ 0.14495812], dtype=float32), 0))\n",
      "(96, (array([ 0.02003365], dtype=float32), 0))\n",
      "(97, (array([ 0.16640894], dtype=float32), 0))\n",
      "(98, (array([ 0.12133495], dtype=float32), 0))\n",
      "(99, (array([ 0.02791455], dtype=float32), 0))\n",
      "(100, (array([ 0.15817243], dtype=float32), 0))\n",
      "(101, (array([ 0.01036706], dtype=float32), 0))\n",
      "(102, (array([ 0.00995322], dtype=float32), 1))\n",
      "(103, (array([ 0.06320418], dtype=float32), 0))\n",
      "(104, (array([ 0.01534594], dtype=float32), 0))\n",
      "(105, (array([ 0.05188472], dtype=float32), 0))\n",
      "(106, (array([ 0.0085216], dtype=float32), 0))\n",
      "(107, (array([ 0.11590829], dtype=float32), 0))\n",
      "(108, (array([ 0.03920348], dtype=float32), 0))\n",
      "(109, (array([ 0.03213551], dtype=float32), 0))\n",
      "(110, (array([ 0.98339903], dtype=float32), 1))\n",
      "(111, (array([ 0.47683179], dtype=float32), 0))\n",
      "(112, (array([ 0.00317858], dtype=float32), 0))\n",
      "(113, (array([ 0.13732214], dtype=float32), 0))\n",
      "(114, (array([ 0.1123435], dtype=float32), 0))\n",
      "(115, (array([ 0.13471146], dtype=float32), 0))\n",
      "(116, (array([ 0.22679909], dtype=float32), 0))\n",
      "(117, (array([ 0.04981223], dtype=float32), 0))\n",
      "(118, (array([ 0.04005273], dtype=float32), 0))\n",
      "(119, (array([ 0.14495882], dtype=float32), 0))\n",
      "(120, (array([ 0.00501862], dtype=float32), 0))\n",
      "(121, (array([ 0.00202631], dtype=float32), 0))\n",
      "(122, (array([ 0.15227503], dtype=float32), 0))\n",
      "(123, (array([ 0.01706651], dtype=float32), 0))\n",
      "(124, (array([ 0.16809449], dtype=float32), 0))\n",
      "(125, (array([ 0.14266433], dtype=float32), 1))\n",
      "(126, (array([ 0.08217762], dtype=float32), 0))\n",
      "(127, (array([ 0.02311366], dtype=float32), 1))\n",
      "(128, (array([ 0.14772803], dtype=float32), 0))\n",
      "(129, (array([ 0.14852649], dtype=float32), 0))\n",
      "(130, (array([ 0.0867568], dtype=float32), 0))\n",
      "(131, (array([ 0.01925778], dtype=float32), 0))\n",
      "(132, (array([ 0.05468876], dtype=float32), 0))\n",
      "(133, (array([ 0.07225147], dtype=float32), 1))\n",
      "(134, (array([ 0.01617267], dtype=float32), 0))\n",
      "(135, (array([ 0.20072195], dtype=float32), 0))\n",
      "(136, (array([ 0.07087499], dtype=float32), 0))\n",
      "(137, (array([ 0.05851815], dtype=float32), 1))\n",
      "(138, (array([ 0.14118682], dtype=float32), 0))\n",
      "(139, (array([ 0.05955327], dtype=float32), 0))\n",
      "(140, (array([ 0.63521713], dtype=float32), 0))\n",
      "(141, (array([ 0.0900031], dtype=float32), 0))\n",
      "(142, (array([ 0.00479957], dtype=float32), 0))\n",
      "(143, (array([ 0.09564012], dtype=float32), 0))\n",
      "(144, (array([ 0.0059837], dtype=float32), 0))\n",
      "(145, (array([ 0.083051], dtype=float32), 0))\n",
      "(146, (array([ 0.02881557], dtype=float32), 0))\n",
      "(147, (array([ 0.99492681], dtype=float32), 1))\n",
      "(148, (array([ 0.06277706], dtype=float32), 0))\n",
      "(149, (array([ 0.61279881], dtype=float32), 1))\n",
      "(150, (array([ 0.03691524], dtype=float32), 0))\n",
      "(151, (array([ 0.0257655], dtype=float32), 1))\n",
      "(152, (array([ 0.02592053], dtype=float32), 0))\n",
      "(153, (array([ 0.57506132], dtype=float32), 1))\n",
      "(154, (array([ 0.19800286], dtype=float32), 0))\n",
      "(155, (array([ 0.26449117], dtype=float32), 1))\n",
      "(156, (array([ 0.14379199], dtype=float32), 0))\n",
      "(157, (array([ 0.14920714], dtype=float32), 0))\n",
      "(158, (array([ 0.00268842], dtype=float32), 0))\n",
      "(159, (array([ 0.05815553], dtype=float32), 0))\n",
      "(160, (array([ 0.1141866], dtype=float32), 1))\n",
      "(161, (array([ 0.05859917], dtype=float32), 0))\n",
      "(162, (array([ 0.00149518], dtype=float32), 0))\n",
      "(163, (array([ 0.28058222], dtype=float32), 0))\n",
      "(164, (array([ 0.05922531], dtype=float32), 0))\n",
      "(165, (array([ 0.05996899], dtype=float32), 0))\n",
      "(166, (array([ 0.14167941], dtype=float32), 0))\n",
      "(167, (array([ 0.01514585], dtype=float32), 0))\n",
      "(168, (array([ 0.1434785], dtype=float32), 0))\n",
      "(169, (array([ 0.16174248], dtype=float32), 0))\n",
      "(170, (array([ 0.13848065], dtype=float32), 0))\n",
      "(171, (array([ 0.05668398], dtype=float32), 0))\n",
      "(172, (array([ 0.19937107], dtype=float32), 0))\n",
      "(173, (array([ 0.91312188], dtype=float32), 0))\n",
      "(174, (array([ 0.02628902], dtype=float32), 0))\n",
      "(175, (array([ 0.13945091], dtype=float32), 0))\n",
      "(176, (array([ 0.02128997], dtype=float32), 0))\n",
      "(177, (array([ 0.01191829], dtype=float32), 0))\n",
      "(178, (array([ 0.05823833], dtype=float32), 0))\n",
      "(179, (array([ 0.83201331], dtype=float32), 0))\n",
      "(180, (array([ 0.14905232], dtype=float32), 0))\n",
      "(181, (array([ 0.04503159], dtype=float32), 0))\n",
      "(182, (array([ 0.08185302], dtype=float32), 0))\n",
      "(183, (array([ 0.05979011], dtype=float32), 0))\n",
      "(184, (array([ 0.0365026], dtype=float32), 0))\n",
      "(185, (array([ 0.00499178], dtype=float32), 0))\n",
      "(186, (array([ 0.16135289], dtype=float32), 0))\n",
      "(187, (array([ 0.05701181], dtype=float32), 0))\n",
      "(188, (array([ 0.00135648], dtype=float32), 1))\n",
      "(189, (array([ 0.01715154], dtype=float32), 0))\n",
      "(190, (array([ 0.00060271], dtype=float32), 0))\n",
      "(191, (array([ 0.14341463], dtype=float32), 0))\n",
      "(192, (array([ 0.13915186], dtype=float32), 0))\n",
      "(193, (array([ 0.14258586], dtype=float32), 0))\n",
      "(194, (array([ 0.07563575], dtype=float32), 0))\n",
      "(195, (array([ 0.01407698], dtype=float32), 0))\n",
      "(196, (array([ 0.1485467], dtype=float32), 0))\n",
      "(197, (array([ 0.07950746], dtype=float32), 0))\n",
      "(198, (array([ 0.06487354], dtype=float32), 0))\n",
      "(199, (array([ 0.02697724], dtype=float32), 0))\n",
      "(200, (array([ 0.32035267], dtype=float32), 0))\n",
      "(201, (array([ 0.01006678], dtype=float32), 0))\n",
      "(202, (array([ 0.44368878], dtype=float32), 0))\n",
      "(203, (array([ 0.08125246], dtype=float32), 0))\n",
      "(204, (array([ 0.0027904], dtype=float32), 0))\n",
      "(205, (array([ 0.15039006], dtype=float32), 0))\n",
      "(206, (array([ 0.03319938], dtype=float32), 0))\n",
      "(207, (array([ 0.00919373], dtype=float32), 0))\n",
      "(208, (array([ 0.14283919], dtype=float32), 0))\n",
      "(209, (array([ 0.10303212], dtype=float32), 0))\n",
      "(210, (array([ 0.09904429], dtype=float32), 0))\n",
      "(211, (array([ 0.02137406], dtype=float32), 1))\n",
      "(212, (array([ 0.07390907], dtype=float32), 0))\n",
      "(213, (array([ 0.06486498], dtype=float32), 0))\n",
      "(214, (array([ 0.05805079], dtype=float32), 0))\n",
      "(215, (array([ 0.03924199], dtype=float32), 0))\n",
      "(216, (array([ 0.01201851], dtype=float32), 0))\n",
      "(217, (array([ 0.00220893], dtype=float32), 0))\n",
      "(218, (array([ 0.21478039], dtype=float32), 0))\n",
      "(219, (array([ 0.00256597], dtype=float32), 0))\n",
      "(220, (array([ 0.07998789], dtype=float32), 0))\n",
      "(221, (array([ 0.60023993], dtype=float32), 1))\n",
      "(222, (array([ 0.00266893], dtype=float32), 0))\n",
      "(223, (array([ 0.84456038], dtype=float32), 1))\n",
      "(224, (array([ 0.14855452], dtype=float32), 0))\n",
      "(225, (array([ 0.44158778], dtype=float32), 0))\n",
      "(226, (array([ 0.01008157], dtype=float32), 0))\n",
      "(227, (array([ 0.02629992], dtype=float32), 0))\n",
      "(228, (array([ 0.01457208], dtype=float32), 0))\n",
      "(229, (array([ 0.01721951], dtype=float32), 0))\n",
      "(230, (array([ 0.43586889], dtype=float32), 0))\n",
      "(231, (array([ 0.15524323], dtype=float32), 1))\n",
      "(232, (array([ 0.03754448], dtype=float32), 0))\n",
      "(233, (array([ 0.03217528], dtype=float32), 0))\n",
      "(234, (array([ 0.83224767], dtype=float32), 0))\n",
      "(235, (array([ 0.02464771], dtype=float32), 0))\n",
      "(236, (array([ 0.00019637], dtype=float32), 0))\n",
      "(237, (array([ 0.1409723], dtype=float32), 1))\n",
      "(238, (array([ 0.14825538], dtype=float32), 0))\n",
      "(239, (array([ 0.0737747], dtype=float32), 0))\n",
      "(240, (array([ 0.0228667], dtype=float32), 1))\n",
      "(241, (array([ 0.03537798], dtype=float32), 0))\n",
      "(242, (array([ 0.26386809], dtype=float32), 1))\n",
      "(243, (array([ 0.03524121], dtype=float32), 0))\n",
      "(244, (array([ 0.06246893], dtype=float32), 0))\n",
      "(245, (array([ 0.14006561], dtype=float32), 0))\n",
      "(246, (array([ 0.00967815], dtype=float32), 0))\n",
      "(247, (array([ 0.61406612], dtype=float32), 0))\n",
      "(248, (array([ 0.00109021], dtype=float32), 0))\n",
      "(249, (array([ 0.06745727], dtype=float32), 0))\n",
      "(250, (array([ 0.12990662], dtype=float32), 0))\n",
      "(251, (array([ 0.13815281], dtype=float32), 0))\n",
      "(252, (array([ 0.17420165], dtype=float32), 0))\n",
      "(253, (array([ 0.00267733], dtype=float32), 0))\n",
      "(254, (array([ 0.140467], dtype=float32), 0))\n",
      "(255, (array([ 0.00728066], dtype=float32), 0))\n",
      "(256, (array([ 0.02240999], dtype=float32), 0))\n",
      "(257, (array([ 0.0576952], dtype=float32), 1))\n",
      "(258, (array([ 0.84859455], dtype=float32), 0))\n",
      "(259, (array([ 0.0017682], dtype=float32), 0))\n",
      "(260, (array([ 0.00831736], dtype=float32), 0))\n",
      "(261, (array([ 0.0495267], dtype=float32), 0))\n",
      "(262, (array([ 0.03128678], dtype=float32), 0))\n",
      "(263, (array([ 0.33406454], dtype=float32), 0))\n",
      "(264, (array([ 0.08558941], dtype=float32), 0))\n",
      "(265, (array([ 0.15558875], dtype=float32), 0))\n",
      "(266, (array([ 0.01086774], dtype=float32), 0))\n",
      "(267, (array([ 0.04375949], dtype=float32), 0))\n",
      "(268, (array([ 0.14335652], dtype=float32), 0))\n",
      "(269, (array([ 0.01176126], dtype=float32), 0))\n",
      "(270, (array([ 0.07287668], dtype=float32), 0))\n",
      "(271, (array([ 0.00722754], dtype=float32), 0))\n",
      "(272, (array([ 0.13879189], dtype=float32), 0))\n",
      "(273, (array([ 0.99781466], dtype=float32), 1))\n",
      "(274, (array([ 0.0356601], dtype=float32), 0))\n",
      "(275, (array([ 0.05978762], dtype=float32), 0))\n",
      "(276, (array([ 0.04885473], dtype=float32), 0))\n",
      "(277, (array([ 0.00527666], dtype=float32), 0))\n",
      "(278, (array([ 0.0285794], dtype=float32), 0))\n",
      "(279, (array([ 0.08507971], dtype=float32), 0))\n",
      "(280, (array([ 0.00549034], dtype=float32), 0))\n",
      "(281, (array([ 0.41788349], dtype=float32), 0))\n",
      "(282, (array([ 0.07041728], dtype=float32), 0))\n",
      "(283, (array([ 0.00768858], dtype=float32), 0))\n",
      "(284, (array([ 0.14886495], dtype=float32), 0))\n",
      "(285, (array([ 0.0064514], dtype=float32), 0))\n",
      "(286, (array([ 0.18456866], dtype=float32), 0))\n",
      "(287, (array([ 0.0049204], dtype=float32), 0))\n",
      "(288, (array([ 0.05064678], dtype=float32), 0))\n",
      "(289, (array([ 0.01203571], dtype=float32), 0))\n",
      "(290, (array([ 0.02200269], dtype=float32), 0))\n",
      "(291, (array([ 0.03900864], dtype=float32), 0))\n",
      "(292, (array([ 0.0257211], dtype=float32), 0))\n",
      "(293, (array([ 0.01420694], dtype=float32), 0))\n"
     ]
    }
   ],
   "source": [
    "##20, 34, 38, 44, 46, 49\n",
    "\n",
    "predicts = model.predict(x_test)\n",
    "to_print = zip(predicts, y_test)\n",
    "count = 0 \n",
    "for i in to_print:\n",
    "    print(count, i)\n",
    "    count +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.345783925663\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a41be5a10>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['acc'], label = 'Accuracy')\n",
    "plt.plot(history.history['val_acc'], label = 'Validation')\n",
    "plt.plot(history.history['val_loss'], label = 'Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "print(np.min(history.history['val_loss']))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 721,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[172,  83],\n",
       "       [ 12,  27]])"
      ]
     },
     "execution_count": 721,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###Weights 2\n",
    "evals = [1 if i > 0.1 else 0 for i in predicts]\n",
    "cf = confusion_matrix(y_test, evals)\n",
    "cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[249,   6],\n",
       "       [ 27,  12]])"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###### Weights 1\n",
    "\n",
    "evals = [1 if i > 0.5 else 0 for i in predicts]\n",
    "t_cf = confusion_matrix(y_test, evals)\n",
    "t_cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "one_prediction = model.predict(x_test[20:21])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.82215291]], dtype=float32)"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ericcriteser/anaconda2/envs/deeplearning/lib/python2.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/Users/ericcriteser/anaconda2/envs/deeplearning/lib/python2.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>DistanceFromHome</th>\n",
       "      <th>Education</th>\n",
       "      <th>EnvironmentSatisfaction</th>\n",
       "      <th>HourlyRate</th>\n",
       "      <th>JobInvolvement</th>\n",
       "      <th>JobLevel</th>\n",
       "      <th>JobSatisfaction</th>\n",
       "      <th>NumCompaniesWorked</th>\n",
       "      <th>PercentSalaryHike</th>\n",
       "      <th>...</th>\n",
       "      <th>JobRole_Manufacturing Director</th>\n",
       "      <th>JobRole_Research Director</th>\n",
       "      <th>JobRole_Research Scientist</th>\n",
       "      <th>JobRole_Sales Executive</th>\n",
       "      <th>JobRole_Sales Representative</th>\n",
       "      <th>MaritalStatus_Divorced</th>\n",
       "      <th>MaritalStatus_Married</th>\n",
       "      <th>MaritalStatus_Single</th>\n",
       "      <th>OverTime_No</th>\n",
       "      <th>OverTime_Yes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>0.478571</td>\n",
       "      <td>0.242857</td>\n",
       "      <td>0.275</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.275</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.885714</td>\n",
       "      <td>...</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Age  DistanceFromHome  Education  EnvironmentSatisfaction  \\\n",
       "277  0.478571          0.242857      0.275                     0.05   \n",
       "\n",
       "     HourlyRate  JobInvolvement  JobLevel  JobSatisfaction  \\\n",
       "277        0.23            0.95     0.275             0.05   \n",
       "\n",
       "     NumCompaniesWorked  PercentSalaryHike      ...       \\\n",
       "277                0.15           0.885714      ...        \n",
       "\n",
       "     JobRole_Manufacturing Director  JobRole_Research Director  \\\n",
       "277                            0.05                       0.05   \n",
       "\n",
       "     JobRole_Research Scientist  JobRole_Sales Executive  \\\n",
       "277                        0.05                     0.95   \n",
       "\n",
       "     JobRole_Sales Representative  MaritalStatus_Divorced  \\\n",
       "277                          0.05                    0.95   \n",
       "\n",
       "     MaritalStatus_Married  MaritalStatus_Single  OverTime_No  OverTime_Yes  \n",
       "277                   0.05                  0.05         0.05          0.95  \n",
       "\n",
       "[1 rows x 48 columns]"
      ]
     },
     "execution_count": 541,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###Micro\n",
    "\n",
    "x_modified = x_test[23:24]\n",
    "x_modified['JobSatisfaction'] = 0.05  ##.05\n",
    "x_modified['EnvironmentSatisfaction'] = 0.05 ## .05\n",
    "x_modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.32943761]], dtype=float32)"
      ]
     },
     "execution_count": 542,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model.predict(x_modified)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>DistanceFromHome</th>\n",
       "      <th>Education</th>\n",
       "      <th>EnvironmentSatisfaction</th>\n",
       "      <th>HourlyRate</th>\n",
       "      <th>JobInvolvement</th>\n",
       "      <th>JobLevel</th>\n",
       "      <th>JobSatisfaction</th>\n",
       "      <th>NumCompaniesWorked</th>\n",
       "      <th>PercentSalaryHike</th>\n",
       "      <th>...</th>\n",
       "      <th>JobRole_Manufacturing Director</th>\n",
       "      <th>JobRole_Research Director</th>\n",
       "      <th>JobRole_Research Scientist</th>\n",
       "      <th>JobRole_Sales Executive</th>\n",
       "      <th>JobRole_Sales Representative</th>\n",
       "      <th>MaritalStatus_Divorced</th>\n",
       "      <th>MaritalStatus_Married</th>\n",
       "      <th>MaritalStatus_Single</th>\n",
       "      <th>OverTime_No</th>\n",
       "      <th>OverTime_Yes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>0.328571</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.275</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>...</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Age  DistanceFromHome  Education  EnvironmentSatisfaction  \\\n",
       "259  0.328571              0.95      0.275                     0.65   \n",
       "\n",
       "     HourlyRate  JobInvolvement  JobLevel  JobSatisfaction  \\\n",
       "259        0.58            0.35      0.05             0.35   \n",
       "\n",
       "     NumCompaniesWorked  PercentSalaryHike      ...       \\\n",
       "259                0.05               0.05      ...        \n",
       "\n",
       "     JobRole_Manufacturing Director  JobRole_Research Director  \\\n",
       "259                            0.05                       0.05   \n",
       "\n",
       "     JobRole_Research Scientist  JobRole_Sales Executive  \\\n",
       "259                        0.05                     0.05   \n",
       "\n",
       "     JobRole_Sales Representative  MaritalStatus_Divorced  \\\n",
       "259                          0.05                    0.05   \n",
       "\n",
       "     MaritalStatus_Married  MaritalStatus_Single  OverTime_No  OverTime_Yes  \n",
       "259                   0.05                  0.95         0.95          0.05  \n",
       "\n",
       "[1 rows x 48 columns]"
      ]
     },
     "execution_count": 480,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test[20:21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 728,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###Macro\n",
    "macro_df = x_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 729,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>DistanceFromHome</th>\n",
       "      <th>Education</th>\n",
       "      <th>EnvironmentSatisfaction</th>\n",
       "      <th>HourlyRate</th>\n",
       "      <th>JobInvolvement</th>\n",
       "      <th>JobLevel</th>\n",
       "      <th>JobSatisfaction</th>\n",
       "      <th>NumCompaniesWorked</th>\n",
       "      <th>PercentSalaryHike</th>\n",
       "      <th>...</th>\n",
       "      <th>JobRole_Manufacturing Director</th>\n",
       "      <th>JobRole_Research Director</th>\n",
       "      <th>JobRole_Research Scientist</th>\n",
       "      <th>JobRole_Sales Executive</th>\n",
       "      <th>JobRole_Sales Representative</th>\n",
       "      <th>MaritalStatus_Divorced</th>\n",
       "      <th>MaritalStatus_Married</th>\n",
       "      <th>MaritalStatus_Single</th>\n",
       "      <th>OverTime_No</th>\n",
       "      <th>OverTime_Yes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1041</th>\n",
       "      <td>0.264286</td>\n",
       "      <td>0.178571</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.744286</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.275</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.435714</td>\n",
       "      <td>0.275</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.397143</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.275</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1222</th>\n",
       "      <td>0.178571</td>\n",
       "      <td>0.725000</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.410000</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>0.628571</td>\n",
       "      <td>0.242857</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.422857</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.435714</td>\n",
       "      <td>...</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>0.435714</td>\n",
       "      <td>0.178571</td>\n",
       "      <td>0.275</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.461429</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.275</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.371429</td>\n",
       "      <td>...</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Age  DistanceFromHome  Education  EnvironmentSatisfaction  \\\n",
       "1041  0.264286          0.178571      0.500                     0.95   \n",
       "184   0.800000          0.435714      0.275                     0.95   \n",
       "1222  0.178571          0.725000      0.050                     0.95   \n",
       "67    0.628571          0.242857      0.500                     0.35   \n",
       "220   0.435714          0.178571      0.275                     0.95   \n",
       "\n",
       "      HourlyRate  JobInvolvement  JobLevel  JobSatisfaction  \\\n",
       "1041    0.744286            0.65     0.275             0.05   \n",
       "184     0.397143            0.95     0.275             0.05   \n",
       "1222    0.410000            0.05     0.050             0.65   \n",
       "67      0.422857            0.65     0.500             0.05   \n",
       "220     0.461429            0.65     0.275             0.35   \n",
       "\n",
       "      NumCompaniesWorked  PercentSalaryHike      ...       \\\n",
       "1041                0.05           0.500000      ...        \n",
       "184                 0.15           0.050000      ...        \n",
       "1222                0.15           0.050000      ...        \n",
       "67                  0.25           0.435714      ...        \n",
       "220                 0.85           0.371429      ...        \n",
       "\n",
       "      JobRole_Manufacturing Director  JobRole_Research Director  \\\n",
       "1041                            0.05                       0.05   \n",
       "184                             0.95                       0.05   \n",
       "1222                            0.05                       0.05   \n",
       "67                              0.05                       0.05   \n",
       "220                             0.05                       0.05   \n",
       "\n",
       "      JobRole_Research Scientist  JobRole_Sales Executive  \\\n",
       "1041                        0.05                     0.95   \n",
       "184                         0.05                     0.05   \n",
       "1222                        0.05                     0.05   \n",
       "67                          0.95                     0.05   \n",
       "220                         0.05                     0.05   \n",
       "\n",
       "      JobRole_Sales Representative  MaritalStatus_Divorced  \\\n",
       "1041                          0.05                    0.05   \n",
       "184                           0.05                    0.95   \n",
       "1222                          0.05                    0.05   \n",
       "67                            0.05                    0.95   \n",
       "220                           0.05                    0.05   \n",
       "\n",
       "      MaritalStatus_Married  MaritalStatus_Single  OverTime_No  OverTime_Yes  \n",
       "1041                   0.05                  0.95         0.95          0.05  \n",
       "184                    0.05                  0.05         0.95          0.05  \n",
       "1222                   0.95                  0.05         0.95          0.05  \n",
       "67                     0.05                  0.05         0.95          0.05  \n",
       "220                    0.05                  0.95         0.95          0.05  \n",
       "\n",
       "[5 rows x 48 columns]"
      ]
     },
     "execution_count": 729,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###Macro\n",
    "#macro_df['OverTime_No'] = 0.95#macro_df['OverTime_No']*1.5\n",
    "#macro_df['OverTime_Yes'] = 0.05\n",
    "macro_df['BusinessTravel_Non-Travel'] = 0.95\n",
    "macro_df['BusinessTravel_Travel_Rarely'] = 0.05\n",
    "macro_df['BusinessTravel_Travel_Frequently'] = 0.05\n",
    "macro_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 730,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 730,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###Macro\n",
    "new_predicts = model.predict(macro_df)\n",
    "turnover = []\n",
    "for i in new_predicts:\n",
    "    if i > 0.1:\n",
    "        turnover.append(i)\n",
    "len(turnover)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1171</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1172</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1173</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1174</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1175</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0\n",
       "1171  0\n",
       "1172  0\n",
       "1173  1\n",
       "1174  0\n",
       "1175  0"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1176"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_train = np.array(y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
